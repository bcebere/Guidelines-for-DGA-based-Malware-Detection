# stdlib
from pathlib import Path

# third party
import numpy as np
import pandas as pd
from joblib import Parallel, delayed

# dga_analysis absolute
from dga_analysis.datasets.dataset_altered_benign import STRATEGIES, DatasetAltered
from dga_analysis.datasets.dataset_dgarchive import FAMILIES as DGA_FAMILIES
from dga_analysis.datasets.dataset_dgarchive import DatasetDGArchive
from dga_analysis.datasets.dataset_synthetic_dga import FAMILIES as SYN_FAMILIES
from dga_analysis.datasets.dataset_synthetic_dga import DatasetSyntheticDGA
from dga_analysis.datasets.dataset_tranco import DatasetTranco
from dga_analysis.detection.cnn import CNNClassifier
from dga_analysis.detection.ji import JIClassifier
from dga_analysis.detection.lr import LinearClassifier
from dga_analysis.detection.nn import NeuralNetClassifier
from dga_analysis.detection.recurrent import (
    GRUClassifier,
    LSTMClassifier,
    RNNClassifier,
)
from dga_analysis.detection.rescnn import ResCNNClassifier
from dga_analysis.detection.rf import RFClassifier
from dga_analysis.detection.svm import SVMClassifier
from dga_analysis.detection.transformer import TransformerClassifier
from dga_analysis.detection.tree import DecisionTreeClassifier
from dga_analysis.detection.xgb import XGBoostClassifier
from dga_analysis.utils.evaluation import evaluate_classifier
from dga_analysis.utils.serialization import (
    dataframe_hash,
    load_from_file,
    save_to_file,
)

WORKSPACE = Path("workspace")
WORKSPACE.mkdir(parents=True, exist_ok=True)

MULTICLASS = True

MODELS_2D = [
    # "rf",
    # "lr",
    # "svm",
    "xgb",
    # "tree",
    "nn",
]
MODELS_3D = [
    # "rnn",
    # "lstm",
    # "gru",
    # "transformer",
    # "cnn",
    "rescnn",
    # "ji",
]


def load_model(model: str):
    if model == "rf":
        return RFClassifier()
    elif model == "lr":
        return LinearClassifier()
    elif model == "svm":
        return SVMClassifier()
    elif model == "xgb":
        return XGBoostClassifier()
    elif model == "tree":
        return DecisionTreeClassifier()
    elif model == "nn":
        return NeuralNetClassifier()
    elif model == "cnn":
        return CNNClassifier(
            n_units_emb=256,
            n_iter=1000,
        )
    elif model == "ji":
        return JIClassifier()
    elif model == "transformer":
        return TransformerClassifier(
            n_layers=1,
            n_units_emb=200,
            dropout=0.1,
            n_iter=1000,
        )
    elif model == "lstm":
        return LSTMClassifier(
            n_layers=1,
            n_units_emb=200,
            n_units_hidden=200,
            dropout=0,
            n_iter=1000,
        )
    elif model == "rnn":
        return RNNClassifier(
            n_layers=2,
            n_units_emb=200,
            n_units_hidden=200,
            dropout=0.1,
            n_iter=1000,
        )
    elif model == "gru":
        return GRUClassifier(
            n_layers=2,
            n_units_emb=200,
            n_units_hidden=200,
            dropout=0.1,
            n_iter=1000,
        )
    elif model == "rescnn":
        return ResCNNClassifier(
            n_units_emb=256,
            dropout=0.1,
            n_iter=1000,
        )


def load_tranco(scenario: str, benign_size: int):
    dataset = DatasetTranco(
        sample_size=benign_size,
    )

    if scenario == "stats":
        return dataset.as_statistical(only_2ld=True)
    elif scenario == "embs":
        return dataset.as_embeddings(only_2ld=True)
    else:
        return dataset.raw(only_2ld=True)


def load_dgarchive(scenario: str, dga_source_size: int):  # stats, embs, raw
    dataset = DatasetDGArchive(
        sample_size=len(DGA_FAMILIES) * dga_source_size,
        max_source_size=dga_source_size,
    )

    if scenario == "stats":
        return dataset.as_statistical(only_2ld=True)
    elif scenario == "embs":
        return dataset.as_embeddings(only_2ld=True, max_length=100)
    else:
        return dataset.raw(only_2ld=True)


def load_synthetic_dga(scenario: str, dga_source_size: int):
    dataset = DatasetSyntheticDGA(
        sample_size=len(SYN_FAMILIES) * dga_source_size,
        max_source_size=dga_source_size,
    )

    if scenario == "stats":
        return dataset.as_statistical(only_2ld=True)
    elif scenario == "embs":
        return dataset.as_embeddings(only_2ld=True)
    else:
        return dataset.raw(only_2ld=True)


def load_altered(scenario: str, dga_source_size):
    X = []
    y = []
    for strategy in STRATEGIES:
        dataset = DatasetAltered(
            sample_size=dga_source_size,
            strategy=strategy,
        )

        if scenario == "stats":
            Xlocal, ylocal = dataset.as_statistical(only_2ld=True)
        elif scenario == "embs":
            Xlocal, ylocal = dataset.as_embeddings(only_2ld=True)
        else:
            Xlocal, ylocal = dataset.raw(only_2ld=True)

        X.append(pd.DataFrame(Xlocal))
        y.append(pd.Series(ylocal))
    return pd.concat(X, ignore_index=True), pd.concat(y, ignore_index=True)


def load_dataset(scenario: str, benign_size: int, dga_size: int):
    X, y = load_tranco(scenario, benign_size=benign_size)
    y = pd.Series(y).astype(int).astype(str)
    y[y == "0"] = "benign"

    Xmal1, ymal1 = load_dgarchive(scenario, dga_source_size=dga_size)
    Xmal2, ymal2 = load_synthetic_dga(scenario, dga_source_size=dga_size)
    Xmal3, ymal3 = load_altered(scenario, dga_source_size=dga_size)
    X = pd.concat(
        [
            pd.DataFrame(X),
            pd.DataFrame(Xmal1),
            pd.DataFrame(Xmal2),
            # pd.DataFrame(Xmal3),
        ],
        ignore_index=True,
    )
    y = pd.concat(
        [
            pd.Series(y),
            pd.Series(ymal1),
            pd.Series(ymal2),
            # pd.Series(ymal3)
        ],
        ignore_index=True,
    )

    return X, y


def benchmark_dataset_2d(testname, X, y):
    X = np.asarray(X)
    data_hash = dataframe_hash(pd.DataFrame(X))

    def _test_model(model_name):
        cache_file = (
            WORKSPACE
            / f"benchmarks_{testname}_{model_name}_{data_hash}_{len(np.unique(y.astype(str)))}.v2.bkp"
        )
        if cache_file.exists():
            scores = load_from_file(cache_file)
            print(
                testname,
                model_name,
                len(X),
                scores["str"],
                flush=True,
            )
            return

        try:
            model = load_model(model_name)
            scores = evaluate_classifier(model, X, y)
            save_to_file(cache_file, scores)
            print(
                testname,
                model_name,
                len(X),
                scores["str"],
                flush=True,
            )
        except BaseException as e:
            print("failed", model_name, e, flush=True)

    parallel = Parallel(n_jobs=len(MODELS_2D))

    parallel(delayed(_test_model)(model_name) for model_name in MODELS_2D)


def benchmark_binary(X, y):
    y_bin = y.copy()
    y_bin[y_bin != "benign"] = 1
    y_bin[y_bin == "benign"] = 0
    benchmark_dataset_2d("binary", X, y_bin)


def benchmark_multiclass(X, y):
    benchmark_dataset_2d("multiclass", X, y)


def benchmark_single_family(X, y):
    uniq_labels = y.unique()

    def _process_label(label):
        label_y = y.copy()
        label_y[label_y != label] = 0
        label_y[label_y == label] = 1
        label_y = label_y.astype(int)
        testcase = f"label_{label}"
        benchmark_dataset_2d(testcase, X, label_y)

    parallel = Parallel(n_jobs=10)
    parallel(delayed(_process_label)(label) for label in uniq_labels)


SCENARIOS = ["embs", "stats"]
SCENARIOS = ["embs"]

# for dga_size in [10, 100, 1000, 10000, 20000]:
for dga_size in [200]:
    for benign_size in [20000]:
        for scenario in SCENARIOS:
            X, y = load_dataset(
                scenario=scenario, benign_size=benign_size, dga_size=dga_size
            )
            # benchmark_single_family(X, y)
            benchmark_binary(X, y)
            # benchmark_multiclass(X, y)
