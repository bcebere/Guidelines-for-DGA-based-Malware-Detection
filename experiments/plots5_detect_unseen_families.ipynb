{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278d339b-3e43-4fa7-a9e9-f02d6fbce4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stdlib\n",
    "from pathlib import Path\n",
    "from random import shuffle\n",
    "\n",
    "# third party\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from tldextract import extract\n",
    "from tqdm import tqdm\n",
    "\n",
    "# dga_analysis absolute\n",
    "from dga_analysis.datasets.dataset_altered_benign import STRATEGIES, DatasetAltered\n",
    "from dga_analysis.datasets.dataset_dgarchive import FAMILIES as DGA_FAMILIES\n",
    "from dga_analysis.datasets.dataset_mixed import DatasetMixed\n",
    "from dga_analysis.datasets.dataset_synthetic_dga import FAMILIES as SYN_FAMILIES\n",
    "from dga_analysis.detection.cnn import CNNClassifier\n",
    "from dga_analysis.detection.ji import JIClassifier\n",
    "from dga_analysis.detection.knn import KNNClassifier\n",
    "from dga_analysis.detection.lr import LinearClassifier\n",
    "from dga_analysis.detection.nn import NeuralNetClassifier\n",
    "from dga_analysis.detection.recurrent import (\n",
    "    GRUClassifier,\n",
    "    LSTMClassifier,\n",
    "    RecurrentClassifier,\n",
    "    RNNClassifier,\n",
    ")\n",
    "from dga_analysis.detection.rescnn import ResCNNClassifier\n",
    "from dga_analysis.detection.rf import RFClassifier\n",
    "from dga_analysis.detection.svm import SVMClassifier\n",
    "from dga_analysis.detection.transformer import TransformerClassifier\n",
    "from dga_analysis.detection.tree import DecisionTreeClassifier\n",
    "from dga_analysis.detection.xgb import XGBoostClassifier\n",
    "from dga_analysis.utils.evaluation import evaluate_classifier\n",
    "from dga_analysis.utils.features.examples import ALL\n",
    "from dga_analysis.utils.serialization import (\n",
    "    dataframe_hash,\n",
    "    load_from_file,\n",
    "    save_to_file,\n",
    ")\n",
    "\n",
    "SAMPLE_SIZE = 40000\n",
    "BENIGN_SUBSAMPLES = 20000\n",
    "DGA_SUBSAMPLES = 20000\n",
    "\n",
    "WORKSPACE = Path(\"workspace\")\n",
    "WORKSPACE.mkdir(parents=True, exist_ok=True)\n",
    "# stdlib\n",
    "from random import shuffle\n",
    "\n",
    "MULTICLASS = False\n",
    "\n",
    "MODELS_2D = [\n",
    "    \"rf\",\n",
    "    \"lr\",\n",
    "    # \"svm\",\n",
    "    \"xgb\",\n",
    "    # \"tree\",\n",
    "    # \"nn\",\n",
    "]\n",
    "\n",
    "MODELS_3D = [\n",
    "    # \"rnn\",\n",
    "    # \"lstm\",\n",
    "    # \"gru\",\n",
    "    # \"transformer\",\n",
    "    # \"cnn\",\n",
    "    \"rescnn\",\n",
    "    # \"ji\",\n",
    "]\n",
    "\n",
    "\n",
    "def load_model(model: str):\n",
    "    if model == \"rf\":\n",
    "        return RFClassifier()\n",
    "    elif model == \"lr\":\n",
    "        return LinearClassifier()\n",
    "    elif model == \"svm\":\n",
    "        return SVMClassifier()\n",
    "    elif model == \"xgb\":\n",
    "        return XGBoostClassifier()\n",
    "    elif model == \"tree\":\n",
    "        return DecisionTreeClassifier()\n",
    "    elif model == \"nn\":\n",
    "        return NeuralNetClassifier(\n",
    "            n_layers=2,\n",
    "            n_units_hidden=200,\n",
    "            dropout=0.1,\n",
    "            n_iter=1000,\n",
    "        )\n",
    "    elif model == \"cnn\":\n",
    "        return CNNClassifier(\n",
    "            n_units_emb=256,\n",
    "            n_iter=1000,\n",
    "        )\n",
    "    elif model == \"ji\":\n",
    "        return JIClassifier()\n",
    "    elif model == \"transformer\":\n",
    "        return TransformerClassifier(\n",
    "            n_layers=1,\n",
    "            n_units_emb=200,\n",
    "            dropout=0.1,\n",
    "            n_iter=1000,\n",
    "        )\n",
    "    elif model == \"lstm\":\n",
    "        return LSTMClassifier(\n",
    "            n_layers=1,\n",
    "            n_units_emb=200,\n",
    "            n_units_hidden=200,\n",
    "            dropout=0,\n",
    "            n_iter=1000,\n",
    "        )\n",
    "    elif model == \"rnn\":\n",
    "        return RNNClassifier(\n",
    "            n_layers=2,\n",
    "            n_units_emb=200,\n",
    "            n_units_hidden=200,\n",
    "            dropout=0.1,\n",
    "            n_iter=1000,\n",
    "        )\n",
    "    elif model == \"gru\":\n",
    "        return GRUClassifier(\n",
    "            n_layers=2,\n",
    "            n_units_emb=200,\n",
    "            n_units_hidden=200,\n",
    "            dropout=0.1,\n",
    "            n_iter=1000,\n",
    "        )\n",
    "    elif model == \"rescnn\":\n",
    "        return ResCNNClassifier(\n",
    "            n_units_emb=256,\n",
    "            dropout=0.1,\n",
    "            n_iter=1000,\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplemented(model)\n",
    "\n",
    "\n",
    "# stdlib\n",
    "import time\n",
    "\n",
    "\n",
    "def load_dgarchive_family(\n",
    "    family: str,\n",
    "    scenario: str,\n",
    "    max_benign_source: int = 100000,\n",
    "    max_dga_source: int = 100000,\n",
    "    benign_source: str = \"tranco\",\n",
    "):\n",
    "    start = time.time()\n",
    "    dataset = DatasetMixed(\n",
    "        sample_size=SAMPLE_SIZE,\n",
    "        sources={benign_source: 0.5, \"dgarchive\": 0.5},\n",
    "        multiclass=MULTICLASS,\n",
    "        dga_families=[family],\n",
    "        max_benign_source=max_benign_source,\n",
    "        max_dga_source=max_dga_source,\n",
    "    )\n",
    "\n",
    "    if scenario == \"stats\":\n",
    "        return dataset.as_statistical(only_2ld=True)\n",
    "    elif scenario == \"embs\":\n",
    "        return dataset.as_embeddings(only_2ld=True)\n",
    "    else:\n",
    "        return dataset.raw(only_2ld=True)\n",
    "\n",
    "\n",
    "def load_synthetic_family(\n",
    "    family: str,\n",
    "    scenario: str = \"stats\",\n",
    "    max_benign_source: int = 100000,\n",
    "    max_dga_source: int = 10000,\n",
    "    benign_source: str = \"tranco\",\n",
    "):  # stats, embs, raw\n",
    "    dataset = DatasetMixed(\n",
    "        sample_size=SAMPLE_SIZE,\n",
    "        sources={benign_source: 0.5, \"synthetic_dga\": 0.5},\n",
    "        multiclass=MULTICLASS,\n",
    "        dga_families=[family],\n",
    "        max_benign_source=max_benign_source,\n",
    "        max_dga_source=max_dga_source,\n",
    "    )\n",
    "\n",
    "    if scenario == \"stats\":\n",
    "        return dataset.as_statistical(only_2ld=True)\n",
    "    elif scenario == \"embs\":\n",
    "        return dataset.as_embeddings(only_2ld=True, max_length=100)\n",
    "    else:\n",
    "        return dataset.raw(only_2ld=True)\n",
    "\n",
    "\n",
    "def load_altered_family(\n",
    "    strategy: str,\n",
    "    scenario: str,\n",
    "    max_benign_source: int,\n",
    "    max_dga_source: int,\n",
    "    benign_source: str = \"tranco\",\n",
    "):\n",
    "    dataset = DatasetMixed(\n",
    "        sample_size=SAMPLE_SIZE,\n",
    "        sources={benign_source: 0.5, \"altered_benign\": 0.5},\n",
    "        dga_families=[strategy],\n",
    "        max_benign_source=max_benign_source,\n",
    "        max_dga_source=max_dga_source,\n",
    "    )\n",
    "\n",
    "    if scenario == \"stats\":\n",
    "        return dataset.as_statistical(only_2ld=True)\n",
    "    elif scenario == \"embs\":\n",
    "        return dataset.as_embeddings(only_2ld=True)\n",
    "    else:\n",
    "        return dataset.raw(only_2ld=True)\n",
    "\n",
    "\n",
    "def benchmark_family_2d(\n",
    "    scenario: str, family_train: str, family_test: str, X_train, y_train, X_test, y_test\n",
    "):\n",
    "    if scenario == \"raw\":\n",
    "        models = MODELS_3D\n",
    "    else:\n",
    "        models = MODELS_2D\n",
    "    X_train = np.asarray(X_train)\n",
    "    X_test = np.asarray(X_test)\n",
    "\n",
    "    train_hash = dataframe_hash(pd.DataFrame(X_train))\n",
    "    test_hash = dataframe_hash(pd.DataFrame(X_test))\n",
    "    best_results = None\n",
    "    best_f1 = -1\n",
    "\n",
    "    for model_name in models:\n",
    "        cache_file = (\n",
    "            WORKSPACE\n",
    "            / f\"benchmarks_cross_{model_name}_{train_hash}_{test_hash}_{len(np.unique(y_train))}.v2.bkp\"\n",
    "        )\n",
    "        if cache_file.exists():\n",
    "            scores = load_from_file(cache_file)\n",
    "            current_f1 = scores[\"raw\"][\"f1_score_macro\"][0]\n",
    "            if current_f1 > best_f1:\n",
    "                best_f1 = current_f1\n",
    "                best_results = scores\n",
    "    return best_results\n",
    "\n",
    "\n",
    "# stdlib\n",
    "import time\n",
    "\n",
    "\n",
    "def benchmark_target(train_family, scenario, X_train, y_train):\n",
    "    results = {}\n",
    "\n",
    "    def _process_family(test_family):\n",
    "        if train_family == test_family:\n",
    "            return None, None\n",
    "        start = time.time()\n",
    "\n",
    "        X_test, y_test = load_dgarchive_family(\n",
    "            test_family,\n",
    "            scenario=scenario,\n",
    "            max_benign_source=BENIGN_SUBSAMPLES,\n",
    "            max_dga_source=DGA_SUBSAMPLES,\n",
    "            benign_source=\"cisco\",\n",
    "        )\n",
    "\n",
    "        local_res = benchmark_family_2d(\n",
    "            scenario, train_family, test_family, X_train, y_train, X_test, y_test\n",
    "        )\n",
    "        return test_family, local_res\n",
    "\n",
    "    parallel = Parallel(n_jobs=4)\n",
    "\n",
    "    # for test_family in DGA_FAMILIES:\n",
    "    #     _process_family(test_family)\n",
    "    test_results = parallel(\n",
    "        delayed(_process_family)(\n",
    "            test_family,\n",
    "        )\n",
    "        for test_family in DGA_FAMILIES\n",
    "    )\n",
    "    for test_family, test_res in test_results:\n",
    "        if test_res is None:\n",
    "            continue\n",
    "        print(train_family, test_family, test_res[\"str\"][\"f1_score_macro\"])\n",
    "\n",
    "        results[test_family] = test_res\n",
    "\n",
    "    def _process_altered(test_strategy):\n",
    "        if train_family == test_strategy:\n",
    "            return None, None\n",
    "\n",
    "        start = time.time()\n",
    "        X_test, y_test = load_altered_family(\n",
    "            test_strategy,\n",
    "            scenario=scenario,\n",
    "            max_benign_source=BENIGN_SUBSAMPLES,\n",
    "            max_dga_source=DGA_SUBSAMPLES,\n",
    "            benign_source=\"cisco\",\n",
    "        )\n",
    "        start = time.time()\n",
    "\n",
    "        local_res = benchmark_family_2d(\n",
    "            scenario, train_family, test_strategy, X_train, y_train, X_test, y_test\n",
    "        )\n",
    "        return test_strategy, local_res\n",
    "\n",
    "    test_results = parallel(\n",
    "        delayed(_process_altered)(\n",
    "            test_strategy,\n",
    "        )\n",
    "        for test_strategy in STRATEGIES\n",
    "    )\n",
    "    for test_family, test_res in test_results:\n",
    "        if test_res is None:\n",
    "            continue\n",
    "        print(train_family, test_family, test_res[\"str\"][\"f1_score_macro\"])\n",
    "\n",
    "        results[test_family] = test_res\n",
    "\n",
    "    def _process_syn(test_family):\n",
    "        if train_family == test_family:\n",
    "            return None, None\n",
    "        X_test, y_test = load_synthetic_family(\n",
    "            test_family,\n",
    "            scenario=scenario,\n",
    "            max_benign_source=BENIGN_SUBSAMPLES,\n",
    "            max_dga_source=DGA_SUBSAMPLES,\n",
    "            benign_source=\"cisco\",\n",
    "        )\n",
    "\n",
    "        local_res = benchmark_family_2d(\n",
    "            scenario, train_family, test_family, X_train, y_train, X_test, y_test\n",
    "        )\n",
    "        return test_family, local_res\n",
    "\n",
    "    test_results = parallel(\n",
    "        delayed(_process_syn)(\n",
    "            test_family,\n",
    "        )\n",
    "        for test_family in SYN_FAMILIES\n",
    "    )\n",
    "    for test_family, test_res in test_results:\n",
    "        if test_res is None:\n",
    "            continue\n",
    "        print(train_family, test_family, test_res[\"str\"][\"f1_score_macro\"])\n",
    "\n",
    "        results[test_family] = test_res\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def benchmark_dgarchive_2d(\n",
    "    family: str,\n",
    "    scenario: str,\n",
    "    max_benign_source: int = BENIGN_SUBSAMPLES,\n",
    "    max_dga_source: int = DGA_SUBSAMPLES,\n",
    "):\n",
    "    X_train, y_train = load_dgarchive_family(\n",
    "        family,\n",
    "        scenario=scenario,\n",
    "        max_benign_source=max_benign_source,\n",
    "        max_dga_source=max_dga_source,\n",
    "    )\n",
    "    return benchmark_target(family, scenario, X_train, y_train)\n",
    "\n",
    "\n",
    "def benchmark_synthetic(\n",
    "    family: str,\n",
    "    scenario: str,\n",
    "    max_benign_source: int = BENIGN_SUBSAMPLES,\n",
    "    max_dga_source: int = DGA_SUBSAMPLES,\n",
    "):\n",
    "    X_train, y_train = load_synthetic_family(\n",
    "        family,\n",
    "        scenario=scenario,\n",
    "        max_benign_source=max_benign_source,\n",
    "        max_dga_source=max_dga_source,\n",
    "    )\n",
    "    return benchmark_target(family, scenario, X_train, y_train)\n",
    "\n",
    "\n",
    "def benchmark_altered_2d(\n",
    "    strategy: str,\n",
    "    scenario: str,\n",
    "    max_benign_source: int = BENIGN_SUBSAMPLES,\n",
    "    max_dga_source: int = DGA_SUBSAMPLES,\n",
    "):\n",
    "    X_train, y_train = load_altered_family(\n",
    "        strategy,\n",
    "        scenario=scenario,\n",
    "        max_benign_source=max_benign_source,\n",
    "        max_dga_source=max_dga_source,\n",
    "    )\n",
    "    return benchmark_target(strategy, scenario, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739bfc66-f71b-4e1c-89a2-2ae1a1836ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCENARIOS = [\"stats\", \"embs\", \"raw\"]\n",
    "\n",
    "results_cache_file = Path(\"results/benchmark_unseen_families_cross.bkp\")\n",
    "if results_cache_file.exists():\n",
    "    results = load_from_file(results_cache_file)\n",
    "else:\n",
    "    results = {}\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b2cdb6-1635-47fc-ad9a-84064309d2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for family in tqdm(DGA_FAMILIES):\n",
    "    if family not in results:\n",
    "        results[family] = {}\n",
    "\n",
    "    for scenario in SCENARIOS:\n",
    "        if scenario in results[family]:\n",
    "            print(\"family cached\", family, scenario)\n",
    "            continue\n",
    "        results[family][scenario] = benchmark_dgarchive_2d(\n",
    "            family,\n",
    "            scenario=scenario,\n",
    "        )\n",
    "        save_to_file(results_cache_file, results)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b524ab-5063-4cf2-a2e8-a3015c96b971",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51acef9-6a0f-4071-9702-071b3fed6136",
   "metadata": {},
   "outputs": [],
   "source": [
    "for family in tqdm(SYN_FAMILIES):\n",
    "    if family not in results:\n",
    "        results[family] = {}\n",
    "\n",
    "    for scenario in SCENARIOS:\n",
    "        if scenario in results[family]:\n",
    "            print(\"family cached\", family, scenario)\n",
    "            continue\n",
    "        results[family][scenario] = benchmark_synthetic(\n",
    "            family,\n",
    "            scenario=scenario,\n",
    "        )\n",
    "        save_to_file(results_cache_file, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853f9d24-46ab-4d85-8b94-c2e2a728d31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for strategy in tqdm(STRATEGIES):\n",
    "    if strategy not in results:\n",
    "        results[strategy] = {}\n",
    "    for scenario in SCENARIOS:\n",
    "        if scenario in results[family]:\n",
    "            print(\"family cached\", scenario, scenario)\n",
    "            continue\n",
    "        family_results[scenario] = benchmark_altered_2d(\n",
    "            strategy,\n",
    "            scenario=scenario,\n",
    "        )\n",
    "\n",
    "        save_to_file(results_cache_file, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7891268-7d18-4750-9abf-a7153026c0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(data, test_dga, metric):\n",
    "    score = 0\n",
    "    if (\n",
    "        test_dga in results[train_dga][\"embs\"]\n",
    "        and results[train_dga][\"embs\"][test_dga] is not None\n",
    "    ):\n",
    "        score = results[train_dga][\"embs\"][test_dga][\"raw\"][metric][0]\n",
    "    if (\n",
    "        test_dga in results[train_dga][\"stats\"]\n",
    "        and results[train_dga][\"stats\"][test_dga] is not None\n",
    "    ):\n",
    "        score = max(score, results[train_dga][\"stats\"][test_dga][\"raw\"][metric][0])\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "target_dgas = {}\n",
    "for train_dga in results:\n",
    "    if len(results[train_dga]) == 0:\n",
    "        continue\n",
    "    scenario = list(results[train_dga].keys())[0]\n",
    "\n",
    "    for test_dga in results[train_dga][scenario]:\n",
    "        if test_dga not in target_dgas:\n",
    "            target_dgas[test_dga] = {\n",
    "                \"f1_score_macro\": [],\n",
    "                \"precision_macro\": [],\n",
    "                \"recall_macro\": [],\n",
    "                \"mcc\": [],\n",
    "            }\n",
    "            # prev_best_f1 = target_dgas[test_dga][\"f1_score_macro\"]\n",
    "        f1_score = max(0.5, get_score(results[train_dga], test_dga, \"f1_score_macro\"))\n",
    "        precision_score = max(\n",
    "            0.5, get_score(results[train_dga], test_dga, \"precision_macro\")\n",
    "        )\n",
    "        recall_score = max(0.5, get_score(results[train_dga], test_dga, \"recall_macro\"))\n",
    "        mcc_score = get_score(results[train_dga], test_dga, \"mcc\")\n",
    "\n",
    "        target_dgas[test_dga][\"f1_score_macro\"].append(f1_score)\n",
    "        target_dgas[test_dga][\"precision_macro\"].append(precision_score)\n",
    "        target_dgas[test_dga][\"recall_macro\"].append(recall_score)\n",
    "        target_dgas[test_dga][\"mcc\"].append(mcc_score)\n",
    "\n",
    "scores = []\n",
    "per_metric_scores = []\n",
    "for train_dga in results:\n",
    "    if len(results[train_dga]) == 0:\n",
    "        continue\n",
    "\n",
    "    scenario = list(results[train_dga].keys())[0]\n",
    "\n",
    "    for test_dga in results[train_dga][scenario]:\n",
    "        if results[train_dga][scenario][test_dga] is None:\n",
    "            continue\n",
    "\n",
    "        f1_score = max(0.5, get_score(results[train_dga], test_dga, \"f1_score_macro\"))\n",
    "        precision_score = max(\n",
    "            0.5, get_score(results[train_dga], test_dga, \"precision_macro\")\n",
    "        )\n",
    "        recall_score = max(0.5, get_score(results[train_dga], test_dga, \"recall_macro\"))\n",
    "        mcc_score = get_score(results[train_dga], test_dga, \"mcc\")\n",
    "        # print(train_dga, test_dga, f1_score, mcc_score)\n",
    "        scores.append(\n",
    "            [train_dga, test_dga, f1_score, precision_score, recall_score, mcc_score]\n",
    "        )\n",
    "        per_metric_scores.extend(\n",
    "            [\n",
    "                [train_dga, test_dga, \"F1 score\", f1_score],\n",
    "                [train_dga, test_dga, \"Precision\", precision_score],\n",
    "                [train_dga, test_dga, \"Recall\", recall_score],\n",
    "                # [train_dga, test_dga, \"MCC\", mcc_score],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "scores = pd.DataFrame(\n",
    "    scores, columns=[\"Train DGA\", \"Test DGA\", \"F1 Score\", \"Precision\", \"Recall\", \"MCC\"]\n",
    ")\n",
    "per_metric_scores = pd.DataFrame(\n",
    "    per_metric_scores, columns=[\"Train DGA\", \"Test DGA\", \"Metric\", \"Score\"]\n",
    ")\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7825c2ab-1c10-4b90-80c7-8b1091254435",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dgas\n",
    "per_test_dga = []\n",
    "per_test_mcc_dga = []\n",
    "\n",
    "for test_dga in target_dgas:\n",
    "    per_test_dga.extend(\n",
    "        [\n",
    "            [test_dga, \"F1 score\", np.mean(target_dgas[test_dga][\"f1_score_macro\"])],\n",
    "            [test_dga, \"Precision\", np.mean(target_dgas[test_dga][\"precision_macro\"])],\n",
    "            [test_dga, \"Recall\", np.mean(target_dgas[test_dga][\"recall_macro\"])],\n",
    "            # [test_dga, \"MCC\", np.mean(target_dgas[test_dga][\"mcc\"]),],\n",
    "        ]\n",
    "    )\n",
    "    per_test_mcc_dga.extend(\n",
    "        [\n",
    "            [\n",
    "                test_dga,\n",
    "                \"MCC\",\n",
    "                np.mean(target_dgas[test_dga][\"mcc\"]),\n",
    "            ],\n",
    "        ]\n",
    "    )\n",
    "per_test_dga = pd.DataFrame(per_test_dga, columns=[\"Test DGA\", \"Metric\", \"Score\"])\n",
    "per_test_mcc_dga = pd.DataFrame(\n",
    "    per_test_mcc_dga, columns=[\"Test DGA\", \"Metric\", \"Score\"]\n",
    ")\n",
    "\n",
    "per_test_dga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dbd268-dd43-41ff-ac79-9c41f2cefe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_test_grouped = []\n",
    "\n",
    "for test_dga in target_dgas:\n",
    "    per_test_grouped.extend(\n",
    "        [\n",
    "            [\n",
    "                test_dga,\n",
    "                np.mean(target_dgas[test_dga][\"f1_score_macro\"]),\n",
    "                np.mean(target_dgas[test_dga][\"precision_macro\"]),\n",
    "                np.mean(target_dgas[test_dga][\"recall_macro\"]),\n",
    "                np.mean(target_dgas[test_dga][\"mcc\"]),\n",
    "            ],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "per_test_grouped = pd.DataFrame(\n",
    "    per_test_grouped, columns=[\"Test DGA\", \"F1 Score\", \"Precision\", \"Recall\", \"MCC\"]\n",
    ")\n",
    "\n",
    "per_test_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b0da10-591e-4d93-b987-4ee7ce55ff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_test_grouped[per_test_grouped[\"F1 Score\"] < 0.6][\"Test DGA\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc713290-dd00-4470-b81c-813f7572fd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_test_grouped[per_test_grouped[\"F1 Score\"] > 0.8][\"Test DGA\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0adc2d7-95f3-49f0-b471-4b8d57df6726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# third party\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# libraries & dataset\n",
    "import seaborn as sns\n",
    "\n",
    "DIAGRAMS = Path(\"diagrams\")\n",
    "DIAGRAMS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "FONT_SIZE = 18\n",
    "\n",
    "prop_cycle = plt.rcParams[\"axes.prop_cycle\"]\n",
    "custom_colors = prop_cycle.by_key()[\"color\"]\n",
    "custom_colors.extend(\n",
    "    [\n",
    "        \"#c5b0d5\",\n",
    "        \"#ffc500\",\n",
    "        \"#700548\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def format_func(value, tick_number):\n",
    "    return f\"{value:.1f}\"\n",
    "\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(\n",
    "    1, 2, figsize=(7, 3), gridspec_kw={\"width_ratios\": [3, 1]}\n",
    ")\n",
    "\n",
    "hue_palette = [\n",
    "    custom_colors[0],\n",
    "    custom_colors[1],\n",
    "    custom_colors[3],\n",
    "    custom_colors[11],\n",
    "]\n",
    "hatches = [\"////\", \"\\\\\\\\\\\\\\\\\", \"||||\", \"xxx\"]\n",
    "\n",
    "bp = sns.boxplot(\n",
    "    x=per_test_dga[\"Metric\"],\n",
    "    y=per_test_dga[\"Score\"],\n",
    "    hue=per_test_dga[\"Metric\"],\n",
    "    palette=hue_palette,\n",
    "    # color = custom_colors[1],\n",
    "    # linecolor =  custom_colors[1],\n",
    "    fliersize=7,\n",
    "    # flierprops={ \"marker\": \"o\",  \"markeredgecolor\": custom_colors[3], \"color\": \"white\", \"markerfacecolor\": custom_colors[3], \"alpha\": 0.2},\n",
    "    boxprops={\"facecolor\": \"none\", \"linewidth\": 2, \"alpha\": 0.9, \"hatch\": \"//////\"},\n",
    "    medianprops={\"color\": \"black\", \"linewidth\": 1, \"linestyle\": \"solid\"},\n",
    "    meanline=True,\n",
    "    showmeans=True,\n",
    "    meanprops={\"color\": \"black\", \"linewidth\": 1, \"linestyle\": \"dashdot\"},\n",
    "    ax=ax0,\n",
    ")\n",
    "for i, bar in enumerate(bp.patches):\n",
    "    if not isinstance(bar, matplotlib.patches.PathPatch):\n",
    "        continue\n",
    "    bar.set_hatch(hatches[int(i)])\n",
    "    bar.set_edgecolor(hue_palette[int(i)])\n",
    "\n",
    "bp = sns.boxplot(\n",
    "    x=per_test_mcc_dga[\"Metric\"],\n",
    "    y=per_test_mcc_dga[\"Score\"],\n",
    "    hue=per_test_mcc_dga[\"Metric\"],\n",
    "    # color = custom_colors[1],\n",
    "    # linecolor =  custom_colors[1],\n",
    "    fliersize=7,\n",
    "    # flierprops={ \"marker\": \"o\",  \"markeredgecolor\": custom_colors[3], \"color\": \"white\", \"markerfacecolor\": custom_colors[3], \"alpha\": 0.2},\n",
    "    boxprops={\"facecolor\": \"none\", \"linewidth\": 2, \"alpha\": 0.9, \"hatch\": \"//////\"},\n",
    "    medianprops={\"color\": \"black\", \"linewidth\": 1, \"linestyle\": \"solid\"},\n",
    "    meanline=True,\n",
    "    showmeans=True,\n",
    "    meanprops={\"color\": \"black\", \"linewidth\": 1, \"linestyle\": \"dashdot\"},\n",
    "    ax=ax1,\n",
    ")\n",
    "for i, bar in enumerate(bp.patches):\n",
    "    if not isinstance(bar, matplotlib.patches.PathPatch):\n",
    "        continue\n",
    "    bar.set_hatch(\"xxx\")\n",
    "    bar.set_edgecolor(custom_colors[11])\n",
    "\n",
    "# plt.xlabel(\"Performance Metric\", fontsize = FONT_SIZE)\n",
    "# plt.ylabel(\"Score\", fontsize = FONT_SIZE)\n",
    "\n",
    "# ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "# ax.tick_params(axis='both', which='minor', labelsize=14)\n",
    "# ax.set_ylim(0.5, 1)\n",
    "\n",
    "fig.text(0.55, -0.05, \"Performance Metric\", fontsize=FONT_SIZE, ha=\"center\")\n",
    "ax0.set_ylabel(\"Score\", fontsize=FONT_SIZE)\n",
    "ax1.set_ylabel(None)\n",
    "ax1.set_xlabel(None)\n",
    "ax0.set_xlabel(None)\n",
    "\n",
    "ax0.tick_params(axis=\"both\", which=\"major\", labelsize=14)\n",
    "ax0.tick_params(axis=\"both\", which=\"minor\", labelsize=14)\n",
    "ax1.tick_params(axis=\"both\", which=\"major\", labelsize=14)\n",
    "ax1.tick_params(axis=\"both\", which=\"minor\", labelsize=14)\n",
    "\n",
    "# arrow_props = dict(arrowstyle='<|-|>', color=\"black\", linewidth=1)\n",
    "# arrow_xpos = 1.01\n",
    "# linecolor = \"black\" #custom_colors[7]\n",
    "# plt.text(2.6, 0.9, \"easy\", color=linecolor, fontsize=14, ha='center', rotation=90)\n",
    "# plt.text(2.6, 0.54, \"difficult\", color=linecolor, fontsize=14, ha='center', rotation=90)\n",
    "\n",
    "# plt.annotate('', xy=(arrow_xpos, 1), xytext=(arrow_xpos, 0.5), arrowprops=arrow_props, xycoords=('axes fraction', 'data'), textcoords=('axes fraction', 'data'))\n",
    "# plt.annotate('', xy=(arrow_xpos, 1), xytext=(arrow_xpos, 0.5), arrowprops=arrow_props, xycoords=('axes fraction', 'data'), textcoords=('axes fraction', 'data'))\n",
    "\n",
    "arrow_props = dict(arrowstyle=\"<|-|>\", color=\"black\", linewidth=1)\n",
    "arrow_xpos = 1.05\n",
    "linecolor = \"black\"  # custom_colors[7]\n",
    "\n",
    "ax1.text(0.7, 0.6, \"easy\", color=linecolor, fontsize=14, ha=\"center\", rotation=90)\n",
    "ax1.text(0.7, -0, \"difficult\", color=linecolor, fontsize=14, ha=\"center\", rotation=90)\n",
    "\n",
    "ax1.annotate(\n",
    "    \"\",\n",
    "    xy=(arrow_xpos, 0.8),\n",
    "    xytext=(arrow_xpos, -0.1),\n",
    "    arrowprops=arrow_props,\n",
    "    xycoords=(\"axes fraction\", \"data\"),\n",
    "    textcoords=(\"axes fraction\", \"data\"),\n",
    ")\n",
    "ax1.annotate(\n",
    "    \"\",\n",
    "    xy=(arrow_xpos, 0.8),\n",
    "    xytext=(arrow_xpos, -0.1),\n",
    "    arrowprops=arrow_props,\n",
    "    xycoords=(\"axes fraction\", \"data\"),\n",
    "    textcoords=(\"axes fraction\", \"data\"),\n",
    ")\n",
    "\n",
    "ax0.yaxis.set_major_formatter(ticker.FuncFormatter(format_func))\n",
    "ax1.yaxis.set_major_formatter(ticker.FuncFormatter(format_func))\n",
    "\n",
    "plt.subplots_adjust(hspace=0, wspace=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.legend(\n",
    "#     loc='lower right',\n",
    "#     ncols=1,\n",
    "#     fontsize = 14,\n",
    "# )\n",
    "\n",
    "figname = \"benchmarks_unseen_family_performance\"\n",
    "plt.savefig(DIAGRAMS / f\"{figname}.pdf\", bbox_inches=\"tight\")\n",
    "plt.savefig(DIAGRAMS / f\"{figname}.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018605f3-479a-4bc8-9844-8cea5a724b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.sort_values(\"Test DGA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b5370b-c9c7-4692-99b2-e6f9d305fc6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56e3461-828c-45a7-a6e2-3837a23fa68c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dga",
   "language": "python",
   "name": "dga"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
