# stdlib
from typing import Any, Optional

# third party
import numpy as np
import torch

# dga_analysis absolute
from dga_analysis.common.gan_3d import GAN3D
from dga_analysis.datasets.dataset_tranco import DatasetTranco
from dga_analysis.utils.constants import DEVICE
from dga_analysis.utils.reproducibility import enable_reproducible_results

# dga_analysis relative
from .base import BaseGenerator


class KhaosGenerator(BaseGenerator):
    def __init__(
        self,
        top_ngram_count: int = 5000,
        seq_len: int = 10,
        ngram_depth: int = 6,
        # generator
        n_units_latent: int = 100,
        generator_type: str = "cnn",  # cnn, lstm
        generator_n_iter: int = 500,
        generator_lr: float = 2e-4,
        generator_weight_decay: float = 1e-3,
        # discriminator
        discriminator_type: str = "cnn",  # cnn, lstm
        discriminator_n_iter: int = 1,
        discriminator_lr: float = 2e-4,
        discriminator_weight_decay: float = 1e-3,
        # training
        batch_size: int = 64,
        random_state: int = 0,
        clipping_value: int = 0,
        lambda_gradient_penalty: float = 10,
        device: Any = DEVICE,
        n_iter_min: int = 100,
        n_iter_print: int = 10,
    ) -> None:
        super().__init__()

        enable_reproducible_results(random_state)
        raw_benign_dataset, _ = DatasetTranco().raw()

        # GAN params
        self.n_units_latent = n_units_latent
        self.generator_type = generator_type
        self.generator_n_iter = generator_n_iter
        self.generator_lr = generator_lr
        self.generator_weight_decay = generator_weight_decay
        self.discriminator_type = discriminator_type
        self.discriminator_n_iter = discriminator_n_iter
        self.discriminator_lr = discriminator_lr
        self.discriminator_weight_decay = discriminator_weight_decay
        self.batch_size = batch_size
        self.random_state = random_state
        self.clipping_value = clipping_value
        self.lambda_gradient_penalty = lambda_gradient_penalty
        self.device = device
        self.n_iter_min = n_iter_min
        self.n_iter_print = n_iter_print

        self._benign_dataset = self._cleanup_raw_domains(raw_benign_dataset)

        self._top_ngram_count = top_ngram_count
        self._ngram_depth = ngram_depth
        self._seq_len = seq_len

        self._top_ngram: Optional[list] = None
        self._generator: Optional[GAN3D] = None

    def _cleanup_raw_domains(self, domains: list) -> list:
        return [domain.split(".")[0] for domain in domains]

    def _int_to_char(self, samples):
        if self._top_ngram is None:
            raise RuntimeError("Fit the model first")

        int_to_char = dict((idx, c) for idx, c in enumerate(self._top_ngram))
        new_strs = []
        for sample in samples:
            str = []
            for idx in sample:
                idx = int(idx)
                if int_to_char[idx] == " ":
                    continue
                str.append(int_to_char[idx])
            str1 = "".join(str)
            new_strs.append(str1)
        return new_strs

    def _token_domain_forward(self, name):
        current = 0
        token = []
        while current < len(name):
            if (len(name) - current) > self._ngram_depth:
                num = self._ngram_depth
            else:
                num = len(name) - current
            for i in range(num, -1, -1):
                if name[current : current + i + 1] in self._top_ngram:
                    token.append(name[current : current + i + 1])
                    current = current + i + 1
                    break
        return token

    def _token_domain_backward(self, name):
        current = len(name)
        token = []
        while current > 0:
            if current > self._ngram_depth - 1:
                num = self._ngram_depth
            else:
                num = current
            for i in range(num, -1, -1):
                if name[current - i : current] in self._top_ngram:
                    token.append(name[current - i : current])
                    current = current - i
                    break
        return token

    def _individual_num(self, name):
        num = 0
        for i in name:
            if len(i) == 1:
                num += 1

        return num

    def _bidirectional_maximum_matching(self, forward, backward):
        if len(forward) > len(backward):
            return backward

        if len(forward) < len(backward):
            return forward

        if forward == backward:
            return forward

        if self._individual_num(forward) > self._individual_num(backward):
            return backward
        elif self._individual_num(forward) < self._individual_num(backward):
            return forward
        else:
            return forward

    def _compute_ngram(self, domains):
        n_gram = dict()
        for domain in domains:
            for idx in range(self._ngram_depth):
                current = 0
                while current + idx + 1 < len(domain) + 1:
                    gram = domain[current : current + idx + 1]
                    current += 1
                    if gram in n_gram:
                        n_gram[gram] += 1
                    else:
                        n_gram[gram] = 1

        return n_gram

    def _compute_top_ngram(self, domains: list):
        n_gram = self._compute_ngram(domains)

        top_n_gram = [" "] + [
            k for k, _ in sorted(n_gram.items(), key=lambda item: item[1], reverse=True)
        ]

        self._top_ngram_count = min(self._top_ngram_count, len(top_n_gram))
        return top_n_gram[: self._top_ngram_count]

    def _compute_one_hot_dataset(self):
        one_hot = []
        data1 = []
        max_length = 0
        for domain in self._benign_dataset:
            forward = self._token_domain_forward(domain)
            backward = self._token_domain_backward(domain)
            data = self._bidirectional_maximum_matching(forward, backward)
            if len(data) > max_length:
                max_length = len(data)
            data1.append(data)
            char_to_int = dict((c, i) for i, c in enumerate(self._top_ngram))
            integer_encoded = [char_to_int[char] for char in data]
            i = len(integer_encoded)
            while i < 10:
                integer_encoded.append(0)
                i += 1
            s = (self._seq_len, self._top_ngram_count)
            one_hot1 = np.zeros(s, dtype=torch.LongTensor)
            for (j, k) in zip(one_hot1, integer_encoded):
                j[k] = 1
            one_hot.append(one_hot1)

        one_hot = np.asarray(one_hot, dtype=int)
        return torch.from_numpy(one_hot), max_length

    def fit(self, X: Optional[np.ndarray] = None) -> "KhaosGenerator":
        if X is not None:
            self._benign_dataset = self._cleanup_raw_domains(X)

        self._top_ngram = self._compute_top_ngram(self._benign_dataset)
        dataset, _ = self._compute_one_hot_dataset()

        self._generator = GAN3D(
            n_features=self._top_ngram_count,
            seq_len=self._seq_len,
            n_units_latent=self.n_units_latent,
            # generator
            generator_type=self.generator_type,
            generator_n_iter=self.generator_n_iter,
            generator_lr=self.generator_lr,
            generator_weight_decay=self.generator_weight_decay,
            # discriminator
            discriminator_type=self.discriminator_type,
            discriminator_n_iter=self.discriminator_n_iter,
            discriminator_lr=self.discriminator_lr,
            discriminator_weight_decay=self.discriminator_weight_decay,
            # training
            batch_size=self.batch_size,
            random_state=self.random_state,
            clipping_value=self.clipping_value,
            lambda_gradient_penalty=self.lambda_gradient_penalty,
            device=self.device,
            n_iter_min=self.n_iter_min,
            n_iter_print=self.n_iter_print,
        )

        self._generator.fit(dataset)

        return self

    def generate(self, n_samples: int) -> np.ndarray:
        samples = self._generator.generate(n_samples)
        samples = np.argmax(samples, axis=2)
        synthetic = self._int_to_char(samples)

        return np.asarray(synthetic)

    @staticmethod
    def name() -> str:
        return "khaos"
