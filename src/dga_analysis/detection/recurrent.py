# stdlib
from typing import Any, Tuple

# third party
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader
from tqdm import tqdm

# dga_analysis absolute
import dga_analysis.logger as log
from dga_analysis.utils.constants import DEVICE
from dga_analysis.utils.dga_nn_dataset import DGADataset, tokenize_data

# dga_analysis relative
from .base import BaseDetector


class RecurrentModel(nn.Module):
    def __init__(
        self,
        model_name: str,
        tokenizer: Any,
        n_units_emb: int = 128,
        n_units_hidden: int = 256,
        output_dim: int = 2,
        n_layers: int = 2,
        dropout: float = 0.1,
    ):
        super().__init__()

        # Embedding layer
        self.embedding = nn.Embedding(len(tokenizer.get_vocab()), n_units_emb)
        bidirectional = True

        # Recurrent layer
        if model_name == "lstm":
            self.recurrent = nn.LSTM(
                n_units_emb,
                n_units_hidden,
                num_layers=n_layers,
                bidirectional=bidirectional,
                dropout=dropout,
                batch_first=True,
            )
        elif model_name == "gru":
            self.recurrent = nn.GRU(
                n_units_emb,
                n_units_hidden,
                num_layers=n_layers,
                bidirectional=bidirectional,
                dropout=dropout,
                batch_first=True,
            )
        elif model_name == "rnn":
            self.recurrent = nn.RNN(
                n_units_emb,
                n_units_hidden,
                num_layers=n_layers,
                bidirectional=bidirectional,
                dropout=dropout,
                batch_first=True,
            )
        else:
            raise NotImplementedError(f"unknown recurrent model {model_name}")

        # Dense layer
        self.fc = nn.Sequential(
            nn.Linear(n_units_hidden * 2, n_units_hidden),
            nn.ReLU(),
            nn.Linear(n_units_hidden, output_dim),
        )
        # Activation function
        self.act = nn.Softmax(dim=-1)

    def forward(self, text: torch.Tensor) -> torch.Tensor:
        # text = [batch size, max_length]
        embedded = self.embedding(text)

        output, (hidden, hidden2) = self.recurrent(embedded)
        if len(hidden.shape) == 3:
            # concat the final forward and backward hidden state
            hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)
        else:
            hidden = torch.cat((hidden2, hidden), dim=1)

        # hidden = [batch size, hid dim * num directions]
        dense_outputs = self.fc(hidden)

        # Final activation function
        outputs = self.act(dense_outputs)

        return outputs


class RecurrentClassifier(BaseDetector):
    def __init__(
        self,
        model_name: str,
        lr: float = 1e-3,
        n_units_emb: int = 128,
        n_units_hidden: int = 256,
        n_layers: int = 2,
        dropout: float = 0.1,
        n_iter: int = 100,
        batch_size: int = 100,
        device=DEVICE,
        patience: int = 10,
        emb_max_len: int = 50,
    ) -> None:
        self.model_name = model_name
        self.lr = lr
        self.n_units_emb = n_units_emb
        self.n_units_hidden = n_units_hidden
        self.n_layers = n_layers
        self.dropout = dropout
        self.n_iter = n_iter
        self.device = device
        self.criterion = nn.CrossEntropyLoss()
        self.batch_size = batch_size
        self.patience = patience
        self.emb_max_len = emb_max_len

    def fit(self, X: Any, y: Any) -> "RecurrentClassifier":
        return self._fit(X, y)

    def predict(self, X: np.ndarray) -> np.ndarray:
        probas = self.predict_proba(X)

        return np.argmax(probas, -1).squeeze()

    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        self.model.eval()

        X = np.asarray(X).squeeze().tolist()
        batch = tokenize_data(X, max_length=self.emb_max_len, tokenizer=self.tokenizer)
        input_ids = batch["input_ids"]
        attention_mask = batch["attention_mask"]

        input_ids = input_ids.to(self.device)
        attention_mask = attention_mask.to(self.device)

        # Forward pass
        outputs = self.model(input_ids)

        return outputs.cpu().detach().numpy().squeeze()

    @staticmethod
    def name() -> str:
        return "recurrent_nn"

    def _fit(self, X: np.ndarray, y: np.ndarray) -> "RecurrentClassifier":
        output_dim = len(np.unique(y))

        X = np.asarray(X).squeeze()
        y = np.asarray(y).squeeze()

        X_train, X_test, y_train, y_test = train_test_split(X, y)

        # Creating instances of training and validation dataset
        train_set = DGADataset(X_train, y_train, max_length=self.emb_max_len)
        val_set = DGADataset(X_test, y_test, max_length=self.emb_max_len)
        self.tokenizer = train_set.tokenizer

        # Creating instances of training and validation dataloaders
        train_loader = DataLoader(train_set, batch_size=self.batch_size, shuffle=True)
        val_loader = DataLoader(val_set, batch_size=self.batch_size)

        # Instantiate the model
        self.model = RecurrentModel(
            self.model_name,
            self.tokenizer,
            n_units_emb=self.n_units_emb,
            n_units_hidden=self.n_units_hidden,
            output_dim=output_dim,
            n_layers=self.n_layers,
            dropout=self.dropout,
        )
        self.model = self.model.to(self.device)
        self.criterion = self.criterion.to(self.device)

        # Define loss function and optimizer
        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)

        # Training loop
        best_val_loss = 9999
        patience = 0
        for epoch in tqdm(range(self.n_iter)):
            train_loss = self._train_one_epoch(train_loader, optimizer)
            val_loss, _ = self._evaluate_one_epoch(val_loader)
            if best_val_loss <= val_loss:
                patience += 1
            else:
                patience = 0
                best_val_loss = val_loss
            if patience > self.patience:
                log.info("Run out of patience")
                break

            if epoch % 100 == 1:
                log.info(
                    f"Epoch: {epoch+1} | Train loss: {train_loss:.3f} | Val loss: {val_loss:.3f} | patience = {patience}"
                )

    def _train_one_epoch(self, data_loader: DataLoader, optimizer: Any) -> torch.Tensor:
        self.model.train()
        epoch_loss = 0
        epoch_accuracy = 0

        for i, batch in enumerate(data_loader):
            input_ids = batch["input_ids"]
            attention_mask = batch["attention_mask"]
            labels = batch["label"]

            # Move tensors to GPU
            input_ids = input_ids.to(self.device)
            attention_mask = attention_mask.to(self.device)
            labels = labels.to(self.device)

            # Forward pass
            outputs = self.model(input_ids)
            accuracy = (outputs.argmax(dim=1) == labels).float().mean()
            # Compute loss
            loss = self.criterion(outputs, labels)

            # Backward pass and optimization
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()
            epoch_accuracy += accuracy.item()
        return epoch_loss / len(data_loader)

    def _evaluate_one_epoch(
        self, data_loader: DataLoader
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        self.model.eval()
        epoch_loss = 0
        epoch_accuracy = 0

        with torch.no_grad():
            for i, batch in enumerate(data_loader):
                input_ids = batch["input_ids"]
                attention_mask = batch["attention_mask"]
                labels = batch["label"]

                # Move tensors to GPU
                input_ids = input_ids.to(self.device)
                attention_mask = attention_mask.to(self.device)
                labels = labels.to(self.device)

                # Forward pass
                outputs = self.model(input_ids)
                accuracy = (outputs.argmax(dim=1) == labels).float().mean()

                # Compute loss
                loss = self.criterion(outputs, labels)

                epoch_loss += loss.item()
                epoch_accuracy += accuracy.item()

        return epoch_loss / len(data_loader), epoch_accuracy / len(data_loader)


class LSTMClassifier(BaseDetector):
    def __init__(
        self,
        lr: float = 1e-3,
        n_units_emb: int = 128,
        n_units_hidden: int = 256,
        n_layers: int = 2,
        dropout: float = 0.1,
        n_iter: int = 100,
        batch_size: int = 100,
        device=torch.device("cuda" if torch.cuda.is_available() else "cpu"),
        patience: int = 10,
        emb_max_len: int = 50,
    ) -> None:
        self.model = RecurrentClassifier(
            model_name="lstm",
            lr=lr,
            n_units_emb=n_units_emb,
            n_units_hidden=n_units_hidden,
            n_layers=n_layers,
            dropout=dropout,
            n_iter=n_iter,
            batch_size=batch_size,
            device=device,
            patience=patience,
            emb_max_len=emb_max_len,
        )

    def fit(self, X: Any, y: Any) -> "LSTMClassifier":
        self.model.fit(X, y)
        return self

    def predict(self, X: np.ndarray) -> np.ndarray:
        return self.model.predict(X)

    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        return self.model.predict_proba(X)

    @staticmethod
    def name() -> str:
        return "lstm"


class RNNClassifier(BaseDetector):
    def __init__(
        self,
        lr: float = 1e-3,
        n_units_emb: int = 128,
        n_units_hidden: int = 256,
        n_layers: int = 2,
        dropout: float = 0.1,
        n_iter: int = 100,
        batch_size: int = 100,
        device=torch.device("cuda" if torch.cuda.is_available() else "cpu"),
        patience: int = 10,
        emb_max_len: int = 50,
    ) -> None:
        self.model = RecurrentClassifier(
            model_name="rnn",
            lr=lr,
            n_units_emb=n_units_emb,
            n_units_hidden=n_units_hidden,
            n_layers=n_layers,
            dropout=dropout,
            n_iter=n_iter,
            batch_size=batch_size,
            device=device,
            patience=patience,
            emb_max_len=emb_max_len,
        )

    def fit(self, X: Any, y: Any) -> "RNNClassifier":
        self.model.fit(X, y)
        return self

    def predict(self, X: np.ndarray) -> np.ndarray:
        return self.model.predict(X)

    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        return self.model.predict_proba(X)

    @staticmethod
    def name() -> str:
        return "rnn"


class GRUClassifier(BaseDetector):
    def __init__(
        self,
        lr: float = 1e-3,
        n_units_emb: int = 128,
        n_units_hidden: int = 256,
        n_layers: int = 2,
        dropout: float = 0.1,
        n_iter: int = 100,
        batch_size: int = 100,
        device=torch.device("cuda" if torch.cuda.is_available() else "cpu"),
        patience: int = 10,
        emb_max_len: int = 50,
    ) -> None:
        self.model = RecurrentClassifier(
            model_name="gru",
            lr=lr,
            n_units_emb=n_units_emb,
            n_units_hidden=n_units_hidden,
            n_layers=n_layers,
            dropout=dropout,
            n_iter=n_iter,
            batch_size=batch_size,
            device=device,
            patience=patience,
            emb_max_len=emb_max_len,
        )

    def fit(self, X: Any, y: Any) -> "GRUClassifier":
        self.model.fit(X, y)
        return self

    def predict(self, X: np.ndarray) -> np.ndarray:
        return self.model.predict(X)

    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        return self.model.predict_proba(X)

    @staticmethod
    def name() -> str:
        return "gru"
