# stdlib
from typing import Any, List, Tuple

# third party
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader
from tqdm import tqdm

# dga_analysis absolute
import dga_analysis.logger as log
from dga_analysis.utils.dga_nn_dataset import DGADataset, tokenize_data

# dga_analysis relative
from .base import BaseDetector


class CNNModel(nn.Module):
    def __init__(
        self,
        tokenizer: Any,
        n_units_emb: int = 128,
        n_filters: int = 10,
        filter_sizes: List[int] = [3, 5, 7],
        output_dim: int = 2,
        dropout: float = 0.1,
    ):
        super().__init__()
        self.embedding = nn.Embedding(len(tokenizer.get_vocab()), n_units_emb)
        self.convs = nn.ModuleList(
            [
                nn.Conv1d(n_units_emb, n_filters, filter_size)
                for filter_size in filter_sizes
            ]
        )
        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)
        self.act = nn.Softmax(dim=-1)
        self.dropout = nn.Dropout(dropout)

    def forward(self, ids):
        # ids = [batch size, seq len]
        embedded = self.dropout(self.embedding(ids))
        # embedded = [batch size, seq len, embedding dim]
        embedded = embedded.permute(0, 2, 1)
        # embedded = [batch size, embedding dim, seq len]
        conved = [torch.relu(conv(embedded)) for conv in self.convs]
        # conved_n = [batch size, n filters, seq len - filter_sizes[n] + 1]
        pooled = [conv.max(dim=-1).values for conv in conved]
        # pooled_n = [batch size, n filters]
        cat = self.dropout(torch.cat(pooled, dim=-1))
        # cat = [batch size, n filters * len(filter_sizes)]
        prediction = self.act(self.fc(cat))
        # prediction = [batch size, output dim]
        return prediction


class CNNClassifier(BaseDetector):
    def __init__(
        self,
        lr: float = 1e-3,
        n_units_emb: int = 128,
        n_filters: int = 10,
        filter_sizes: List[int] = [3, 5, 7],
        dropout: float = 0.1,
        n_iter: int = 100,
        batch_size: int = 100,
        device=torch.device("cuda" if torch.cuda.is_available() else "cpu"),
        patience: int = 10,
        emb_max_len: int = 50,
    ) -> None:
        self.n_filters = n_filters
        self.filter_sizes = filter_sizes
        self.lr = lr
        self.n_units_emb = n_units_emb
        self.dropout = dropout
        self.n_iter = n_iter
        self.device = device
        self.criterion = nn.CrossEntropyLoss()
        self.batch_size = batch_size
        self.patience = patience
        self.emb_max_len = emb_max_len

    def fit(self, X: Any, y: Any) -> "CNNClassifier":
        return self._fit(X, y)

    def predict(self, X: np.ndarray) -> np.ndarray:
        probas = self.predict_proba(X)

        return np.argmax(probas, -1).squeeze()

    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        self.model.eval()

        X = np.asarray(X).squeeze().tolist()
        batch = tokenize_data(X, max_length=self.emb_max_len, tokenizer=self.tokenizer)
        input_ids = batch["input_ids"]
        attention_mask = batch["attention_mask"]

        input_ids = input_ids.to(self.device)
        attention_mask = attention_mask.to(self.device)

        # Forward pass
        outputs = self.model(input_ids)

        return outputs.cpu().detach().numpy().squeeze()

    @staticmethod
    def name() -> str:
        return "cnn"

    def _fit(self, X: np.ndarray, y: np.ndarray) -> "CNNClassifier":
        output_dim = len(np.unique(y))

        X = np.asarray(X).squeeze()
        y = np.asarray(y).squeeze()

        X_train, X_test, y_train, y_test = train_test_split(X, y)

        # Creating instances of training and validation dataset
        train_set = DGADataset(X_train, y_train, max_length=self.emb_max_len)
        val_set = DGADataset(X_test, y_test, max_length=self.emb_max_len)
        self.tokenizer = train_set.tokenizer

        # Creating instances of training and validation dataloaders
        train_loader = DataLoader(train_set, batch_size=self.batch_size, shuffle=True)
        val_loader = DataLoader(val_set, batch_size=self.batch_size)

        # Instantiate the model
        self.model = CNNModel(
            self.tokenizer,
            n_units_emb=self.n_units_emb,
            n_filters=self.n_filters,
            filter_sizes=self.filter_sizes,
            output_dim=output_dim,
            dropout=self.dropout,
        )
        self.model = self.model.to(self.device)
        self.criterion = self.criterion.to(self.device)

        # Define loss function and optimizer
        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)

        # Training loop
        best_val_loss = 9999
        patience = 0
        for epoch in tqdm(range(self.n_iter)):
            train_loss = self._train_one_epoch(train_loader, optimizer)
            val_loss, _ = self._evaluate_one_epoch(val_loader)
            if best_val_loss <= val_loss:
                patience += 1
            else:
                patience = 0
                best_val_loss = val_loss
            if patience > self.patience:
                log.info("Run out of patience")
                break

            if epoch % 100 == 1:
                log.info(
                    f"Epoch: {epoch+1} | Train loss: {train_loss:.3f} | Val loss: {val_loss:.3f} | patience = {patience}"
                )

    def _train_one_epoch(self, data_loader: DataLoader, optimizer: Any) -> torch.Tensor:
        self.model.train()
        epoch_loss = 0
        epoch_accuracy = 0

        for i, batch in enumerate(data_loader):
            input_ids = batch["input_ids"]
            attention_mask = batch["attention_mask"]
            labels = batch["label"]

            # Move tensors to GPU
            input_ids = input_ids.to(self.device)
            attention_mask = attention_mask.to(self.device)
            labels = labels.to(self.device)

            # Forward pass
            outputs = self.model(input_ids)
            accuracy = (outputs.argmax(dim=1) == labels).float().mean()
            # Compute loss
            loss = self.criterion(outputs, labels)

            # Backward pass and optimization
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()
            epoch_accuracy += accuracy.item()
        return epoch_loss / len(data_loader)

    def _evaluate_one_epoch(
        self, data_loader: DataLoader
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        self.model.eval()
        epoch_loss = 0
        epoch_accuracy = 0

        with torch.no_grad():
            for i, batch in enumerate(data_loader):
                input_ids = batch["input_ids"]
                attention_mask = batch["attention_mask"]
                labels = batch["label"]

                # Move tensors to GPU
                input_ids = input_ids.to(self.device)
                attention_mask = attention_mask.to(self.device)
                labels = labels.to(self.device)

                # Forward pass
                outputs = self.model(input_ids)
                accuracy = (outputs.argmax(dim=1) == labels).float().mean()

                # Compute loss
                loss = self.criterion(outputs, labels)

                epoch_loss += loss.item()
                epoch_accuracy += accuracy.item()

        return epoch_loss / len(data_loader), epoch_accuracy / len(data_loader)
