# stdlib
from typing import Any, Callable, Optional, Tuple

# third party
import numpy as np
import torch
from torch import nn
from torch.utils.data import DataLoader, TensorDataset, sampler
from tqdm import tqdm

# dga_analysis absolute
import dga_analysis.logger as log
from dga_analysis.utils.constants import DEVICE
from dga_analysis.utils.reproducibility import enable_reproducible_results


class ResBlock(nn.Module):
    def __init__(self, n_units_latent: int = 64, n_channels=3, weight: float = 0.3):
        super(ResBlock, self).__init__()

        self.weight = weight
        self.res_block = nn.Sequential(
            nn.ReLU(True),
            nn.Conv1d(n_units_latent, n_units_latent, n_channels, padding=1),
            nn.ReLU(True),
            nn.Conv1d(n_units_latent, n_units_latent, n_channels, padding=1),
        )

    def forward(self, X):
        output = self.res_block(X)
        return X + (self.weight * output)


class Generator(nn.Module):
    def __init__(
        self,
        model_type: str,
        n_features: int,
        n_units_latent: int,
        seq_len: int,
        batch_size: int,
        lr: float = 1e-3,
        n_layers: int = 2,
        weight_decay: float = 1e-3,
    ):
        super(Generator, self).__init__()

        self.fc1 = nn.Linear(n_features, n_units_latent * seq_len)

        if model_type == "cnn":
            self.block = nn.Sequential(
                ResBlock(n_units_latent=n_units_latent),
                ResBlock(n_units_latent=n_units_latent),
                ResBlock(n_units_latent=n_units_latent),
            )
            self.conv1 = nn.Conv1d(n_units_latent, n_features, 1)
        elif model_type == "lstm":
            self.recurrent = nn.LSTM(
                n_features,
                n_units_latent,
                num_layers=n_layers,
                bidirectional=True,
                batch_first=True,
            )
            self.fc = nn.Sequential(
                nn.Linear(2 * n_units_latent, n_units_latent),
                nn.ReLU(),
                nn.Linear(n_units_latent, seq_len * n_features),
            )

        else:
            raise ValueError(f"unknown model type {model_type}")

        self.model_type = model_type
        self.batch_size = batch_size
        self.seq_len = seq_len
        self.n_features = n_features
        self.n_units_latent = n_units_latent

        # optimizer
        self.lr = lr
        self.weight_decay = weight_decay
        self.optimizer = torch.optim.Adam(
            self.parameters(),
            lr=self.lr,
            weight_decay=self.weight_decay,
        )

    def forward(self, noise):
        output = self.fc1(noise)
        if self.model_type == "cnn":
            output = output.view(-1, self.n_units_latent, self.seq_len)
            output = self.block(output)
            output = self.conv1(output)
            output = output.transpose(1, 2)
            output = output.contiguous()
            output = output.view(output.shape[0] * self.seq_len, -1)
        elif self.model_type == "lstm":
            output = output.view(-1, self.seq_len, self.n_units_latent)
            _, (hidden, _) = self.recurrent(output)
            output = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)
            output = self.fc(output)
        else:
            raise ValueError(f"Unknwon model type {self.model_type}")

        output = nn.Softmax(dim=-1)(output)
        return output.view(
            (noise.shape[0], self.seq_len, self.n_features)
        )  # (BATCH_SIZE, SEQ_LEN, len(charmap))


class Discriminator(nn.Module):
    def __init__(
        self,
        model_type: str,
        n_features: int,
        n_units_latent: int,
        seq_len: int,
        lr: float = 1e-3,
        n_layers: int = 1,
        weight_decay: float = 1e-3,
    ):
        super(Discriminator, self).__init__()

        if model_type == "cnn":
            self.conv1d = nn.Conv1d(n_features, n_units_latent, 1)
            self.block = nn.Sequential(
                ResBlock(n_units_latent=n_units_latent),
                ResBlock(n_units_latent=n_units_latent),
                ResBlock(n_units_latent=n_units_latent),
            )
            self.fc = nn.Linear(seq_len * n_units_latent, 1)
        elif model_type == "lstm":
            self.recurrent = nn.LSTM(
                n_features,
                n_units_latent,
                num_layers=n_layers,
                bidirectional=True,
                batch_first=True,
            )
            self.fc = nn.Sequential(
                nn.Linear(2 * n_units_latent, n_units_latent),
                nn.ReLU(),
                nn.Linear(n_units_latent, 1),
            )
        else:
            raise ValueError(f"Unknown model type {model_type}")

        self.model_type = model_type

        self.seq_len = seq_len
        self.n_units_latent = n_units_latent

        # optimizer
        self.lr = lr
        self.weight_decay = weight_decay
        self.optimizer = torch.optim.Adam(
            self.parameters(),
            lr=self.lr,
            weight_decay=self.weight_decay,
        )

    def forward(self, input):
        if self.model_type == "cnn":
            output = input.transpose(1, 2)  # (BATCH_SIZE, len(charmap), SEQ_LEN)
            output = self.conv1d(output)
            output = self.block(output)
            output = output.view(-1, self.seq_len * self.n_units_latent)
        elif self.model_type == "lstm":
            _, (hidden, _) = self.recurrent(input)
            output = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)
        else:
            raise ValueError(f"Unknown model type {self.model_type}")

        return self.fc(output)


class GAN3D(nn.Module):
    """
    Basic GAN implementation.
    """

    def __init__(
        self,
        n_features: int,
        seq_len: int,
        n_units_latent: int,
        # generator
        generator_type: str = "cnn",  # cnn, lstm
        generator_n_iter: int = 500,
        generator_lr: float = 2e-4,
        generator_weight_decay: float = 1e-3,
        # discriminator
        discriminator_type: str = "cnn",  # cnn, lstm
        discriminator_n_iter: int = 1,
        discriminator_lr: float = 2e-4,
        discriminator_weight_decay: float = 1e-3,
        discriminator_opt_betas: tuple = (0.9, 0.999),
        # training
        batch_size: int = 64,
        random_state: int = 0,
        clipping_value: int = 0,
        lambda_gradient_penalty: float = 10,
        dataloader_sampler: Optional[sampler.Sampler] = None,
        device: Any = DEVICE,
        n_iter_min: int = 100,
        n_iter_print: int = 10,
    ) -> None:
        super(GAN3D, self).__init__()

        assert discriminator_type in ["cnn", "lstm"]

        log.info(f"Training GAN on device {device}. features = {n_features}")
        self.device = device

        self.n_features = n_features
        self.n_units_latent = n_units_latent

        self.generator = Generator(
            model_type=generator_type,
            n_features=n_features,
            n_units_latent=n_units_latent,
            seq_len=seq_len,
            batch_size=batch_size,
            lr=generator_lr,
            weight_decay=generator_weight_decay,
        ).to(self.device)

        self.discriminator = Discriminator(
            model_type=discriminator_type,
            n_features=n_features,
            n_units_latent=n_units_latent,
            seq_len=seq_len,
            lr=discriminator_lr,
            weight_decay=discriminator_weight_decay,
        ).to(self.device)

        # training
        self.generator_n_iter = generator_n_iter
        self.discriminator_n_iter = discriminator_n_iter
        self.n_iter_print = n_iter_print
        self.n_iter_min = n_iter_min
        self.batch_size = batch_size
        self.clipping_value = clipping_value

        self.lambda_gradient_penalty = lambda_gradient_penalty

        self.random_state = random_state
        enable_reproducible_results(random_state)

        def gen_fake_labels(X: torch.Tensor) -> torch.Tensor:
            return torch.zeros((len(X),), device=self.device)

        def gen_true_labels(X: torch.Tensor) -> torch.Tensor:
            return torch.ones((len(X),), device=self.device)

        self.fake_labels_generator = gen_fake_labels
        self.true_labels_generator = gen_true_labels
        self.dataloader_sampler = dataloader_sampler

    def fit(
        self,
        X: np.ndarray,
        fake_labels_generator: Optional[Callable] = None,
        true_labels_generator: Optional[Callable] = None,
    ) -> "GAN3D":
        Xt = self._check_tensor(X)

        self._train(
            Xt,
            fake_labels_generator=fake_labels_generator,
            true_labels_generator=true_labels_generator,
        )

        return self

    def generate(self, count: int) -> np.ndarray:
        self.generator.eval()
        with torch.no_grad():
            return self(count).detach().cpu().numpy()

    def forward(
        self,
        count: int,
    ) -> torch.Tensor:
        fixed_noise = torch.randn(count, self.n_features, device=self.device)
        return self.generator(fixed_noise)

    def dataloader(
        self,
        X: torch.Tensor,
    ) -> DataLoader:
        dataset = TensorDataset(X)
        return DataLoader(
            dataset,
            batch_size=self.batch_size,
            sampler=self.dataloader_sampler,
            pin_memory=False,
        )

    def _train_epoch_generator(
        self,
        X: torch.Tensor,
        fake_labels_generator: Callable,
        true_labels_generator: Callable,
    ) -> float:
        # Update the G network
        self.generator.train()
        self.generator.optimizer.zero_grad()

        real_X = X.to(self.device)
        batch_size = len(real_X)

        noise = torch.randn(batch_size, self.n_features, device=self.device)

        fake = self.generator(noise)

        output = self.discriminator(fake).squeeze().float()
        # Calculate G's loss based on this output
        errG = -torch.mean(output)

        # Calculate gradients for G
        errG.backward()

        # Update G
        if self.clipping_value > 0:
            torch.nn.utils.clip_grad_norm_(
                self.generator.parameters(), self.clipping_value
            )
        self.generator.optimizer.step()

        if torch.isnan(errG):
            raise RuntimeError("NaNs detected in the generator loss")

        # Return loss
        return errG.item()

    def _train_epoch_discriminator(
        self,
        X: torch.Tensor,
        fake_labels_generator: Callable,
        true_labels_generator: Callable,
    ) -> float:
        # Update the D network
        self.discriminator.train()

        errors = []

        batch_size = min(self.batch_size, len(X))

        for epoch in range(self.discriminator_n_iter):
            # Train with all-real batch
            real_X = X.to(self.device)

            real_labels = true_labels_generator(X).to(self.device).squeeze()
            real_output = self.discriminator(real_X).squeeze().float()

            # Train with all-fake batch
            noise = torch.randn(batch_size, self.n_features, device=self.device)

            fake = self.generator(noise)

            fake_labels = fake_labels_generator(fake).to(self.device).squeeze().float()
            fake_output = self.discriminator(fake.detach()).squeeze()

            # Compute errors. Some fake inputs might be marked as real for privacy guarantees.

            real_real_output = real_output[(real_labels * real_output) != 0]
            real_fake_output = fake_output[(fake_labels * fake_output) != 0]
            errD_real = torch.mean(torch.concat((real_real_output, real_fake_output)))

            fake_real_output = real_output[((1 - real_labels) * real_output) != 0]
            fake_fake_output = fake_output[((1 - fake_labels) * fake_output) != 0]
            errD_fake = torch.mean(torch.concat((fake_real_output, fake_fake_output)))

            penalty = self._loss_gradient_penalty(
                real_samples=real_X,
                fake_samples=fake,
                batch_size=batch_size,
            )
            errD = -errD_real + errD_fake

            self.discriminator.optimizer.zero_grad()
            penalty.backward(retain_graph=True)
            errD.backward()

            # Update D
            if self.clipping_value > 0:
                torch.nn.utils.clip_grad_norm_(
                    self.discriminator.parameters(), self.clipping_value
                )
            self.discriminator.optimizer.step()

            errors.append(errD.item())

        if np.isnan(np.mean(errors)):
            raise RuntimeError("NaNs detected in the discriminator loss")

        return np.mean(errors)

    def _train_epoch(
        self,
        loader: DataLoader,
        fake_labels_generator: Optional[Callable] = None,
        true_labels_generator: Optional[Callable] = None,
    ) -> Tuple[float, float]:
        if fake_labels_generator is None:
            fake_labels_generator = self.fake_labels_generator
        if true_labels_generator is None:
            true_labels_generator = self.true_labels_generator

        G_losses = []
        D_losses = []

        for i, data in enumerate(loader):
            X = data[0]

            D_losses.append(
                self._train_epoch_discriminator(
                    X,
                    fake_labels_generator=fake_labels_generator,
                    true_labels_generator=true_labels_generator,
                )
            )
            G_losses.append(
                self._train_epoch_generator(
                    X,
                    fake_labels_generator=fake_labels_generator,
                    true_labels_generator=true_labels_generator,
                )
            )

        return np.mean(G_losses), np.mean(D_losses)

    def _train(
        self,
        X: torch.Tensor,
        fake_labels_generator: Optional[Callable] = None,
        true_labels_generator: Optional[Callable] = None,
    ) -> "GAN3D":
        X = self._check_tensor(X).float()

        # Load Dataset
        loader = self.dataloader(X)

        # Train loop
        for i in tqdm(range(self.generator_n_iter)):
            g_loss, d_loss = self._train_epoch(
                loader,
                fake_labels_generator=fake_labels_generator,
                true_labels_generator=true_labels_generator,
            )
            # Check how the generator is doing by saving G's output on fixed_noise
            if (i + 1) % self.n_iter_print == 0:
                log.debug(
                    f"[{i}/{self.generator_n_iter}]\tLoss_D: {d_loss}\tLoss_G: {g_loss}"
                )

        return self

    def _check_tensor(self, X: torch.Tensor) -> torch.Tensor:
        if isinstance(X, torch.Tensor):
            return X.to(self.device)
        else:
            return torch.from_numpy(np.asarray(X)).to(self.device)

    def _loss_gradient_penalty(
        self,
        real_samples: torch.tensor,
        fake_samples: torch.Tensor,
        batch_size: int,
    ) -> torch.Tensor:
        """Calculates the gradient penalty loss for WGAN GP"""
        # Random weight term for interpolation between real and fake samples
        alpha = torch.rand([batch_size, 1, 1]).to(self.device)
        # Get random interpolation between real and fake samples
        interpolated = (
            alpha * real_samples + ((1 - alpha) * fake_samples)
        ).requires_grad_(True)
        d_interpolated = self.discriminator(interpolated).squeeze()
        labels = torch.ones((len(interpolated),), device=self.device)

        # Get gradient w.r.t. interpolates
        gradients = torch.autograd.grad(
            outputs=d_interpolated,
            inputs=interpolated,
            grad_outputs=labels,
            create_graph=True,
            retain_graph=True,
            only_inputs=True,
            allow_unused=True,
        )[0]
        gradient_penalty = ((gradients.norm(2, dim=-1) - 1) ** 2).mean()
        return self.lambda_gradient_penalty * gradient_penalty
