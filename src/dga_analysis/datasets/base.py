# stdlib
from abc import ABCMeta, abstractmethod
from os.path import dirname
from pathlib import Path
from typing import List, Tuple

# third party
import nltk
import numpy as np
import pandas as pd
from nltk.util import ngrams
from scipy.sparse import coo_matrix
from sklearn.preprocessing import LabelEncoder
from tldextract import extract

# dga_analysis absolute
from dga_analysis.utils.dga_nn_dataset import tokenize_data
from dga_analysis.utils.extraction import FeatureExtraction
from dga_analysis.utils.features.examples import ALL
from dga_analysis.utils.reproducibility import enable_reproducible_results
from dga_analysis.utils.serialization import dataframe_hash

nltk.download("punkt")


DYNDNS = [
    "co",
    "cz",
    "ddns",
    "dyndns",
    "dynserv",
    "mooo",
    "yi",
    "hk",
    "south",
    "netcn",
    "ru",
    "uy",
    "github",
]


class BaseDataset(metaclass=ABCMeta):
    """Base class for the datasets."""

    def __init__(
        self,
        seed=0,
        workspace=Path(dirname(__file__)),
    ) -> None:
        super().__init__()

        self.workspace = workspace
        enable_reproducible_results(seed)
        self._load()

    @abstractmethod
    def _load(self) -> "BaseDataset":
        ...

    @abstractmethod
    def raw(self, only_2ld: bool = False, remove_duplicates: bool = False) -> Tuple[List, List]:
        ...

    @staticmethod
    def get_2ld(domain: str) -> str:
        tokens = extract(domain)
        token = tokens.domain
        if token in DYNDNS and tokens.subdomain != "":
            token = tokens.subdomain

        if token == "":
            return domain
        return token

    def _encode_labels(self, raw_labels):
        label_counts = pd.Series(raw_labels).value_counts()
        rare_labels = label_counts[label_counts <= 5].index.values.tolist()
        raw_labels = pd.Series(raw_labels)
        raw_labels[raw_labels.isin(rare_labels)] = "other"
        raw_labels = pd.Series(raw_labels).astype(str)
        raw_labels = LabelEncoder().fit_transform(raw_labels)

        return raw_labels

    def raw_df(self, only_2ld: bool = False, remove_duplicates: bool = False) -> Tuple[List, List]:
        raw_data, raw_labels = self.raw(only_2ld=only_2ld, remove_duplicates=remove_duplicates)
        raw_labels = self._encode_labels(raw_labels)
        cache = pd.DataFrame({"domain": raw_data, "label": raw_labels})
        if remove_duplicates:
            cache = cache.drop_duplicates()

        return cache

    def raw_np(self, only_2ld: bool = False, remove_duplicates: bool = False) -> Tuple[List, List]:
        df = self.raw_df(only_2ld=only_2ld, remove_duplicates=remove_duplicates)

        return df["domain"].values, df["label"].values

    def as_statistical(
        self, fillna=True, only_2ld: bool = True, remove_duplicates: bool = False
    ) -> Tuple[pd.DataFrame, pd.Series]:
        """
        The output might contains NaNs!
        """
        df = self.raw_df(only_2ld=only_2ld, remove_duplicates=remove_duplicates)

        df_hash = self.hash(only_2ld)

        data_path = self.workspace / ".data/stats"
        data_path.mkdir(parents=True, exist_ok=True)

        cache_path = data_path / f"{df_hash}_{only_2ld}.csv"

        if cache_path.exists():
            X = pd.read_csv(cache_path)
        else:
            fe = FeatureExtraction(features=ALL, n_jobs=4, verbose=False)
            raw_data = [domain.lower() for domain in df["domain"]]

            fe.fit()
            X = fe.transform(raw_data)
            X = pd.DataFrame.sparse.from_spmatrix(coo_matrix(X))
            X.columns = fe.get_feature_names()

            X.to_csv(cache_path, index=None)

        if fillna:
            X = X.fillna(-1)

        return X, df["label"]

    def as_ngrams(self, only_2ld: bool = True, remove_duplicates: bool = False, ngram_len: int = 2):
        raw_data, raw_labels = self.raw(only_2ld, remove_duplicates=remove_duplicates)
        ngram_list = []
        for string in raw_data:
            string_ngrams = list(ngrams(string, ngram_len))
            string_ngrams = ["".join(raw_ngram) for raw_ngram in string_ngrams]
            ngram_list.append(string_ngrams)
        return ngram_list, raw_labels

    def as_embeddings(self, only_2ld: bool = True, remove_duplicates: bool = False, max_length: int = 100):
        raw_data, labels = self.raw(only_2ld=only_2ld, remove_duplicates=remove_duplicates)
        embs = tokenize_data(raw_data, max_length=max_length)["input_ids"]
        embs = np.asarray(embs)

        return embs, labels

    # helpers
    def covariates(self, only_2ld: bool = False, remove_duplicates: bool = False) -> List:
        raw_data, raw_labels = self.raw(only_2ld=only_2ld, remove_duplicates=remove_duplicates)
        return np.asarray(raw_data)

    def covariates_statistical(
        self, only_2ld: bool = False, remove_duplicates: bool = False, as_numpy: bool = False
    ) -> pd.DataFrame:
        stats, _ = self.as_statistical(only_2ld=only_2ld, remove_duplicates=remove_duplicates)
        if as_numpy:
            return np.asarray(stats)
        return stats

    def hash(self, only_2ld: bool = False, remove_duplicates: bool = False) -> List:
        raw_df = self.raw_df(only_2ld=only_2ld, remove_duplicates=remove_duplicates)
        return dataframe_hash(raw_df)

    @staticmethod
    def _drop_duplicates(samples, labels, remove_duplicates: bool = True) -> (list, list):
        if not remove_duplicates:
            return samples, labels
        unique_samples, unique_labels = zip(*{(sample, label) for sample, label in zip(samples, labels)})
        return unique_samples, unique_labels

    def __len__(self):
        return len(self.covariates())
