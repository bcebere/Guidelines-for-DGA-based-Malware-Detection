# stdlib
from abc import abstractmethod
from pathlib import Path
from typing import Any, Callable, Dict, Tuple

# third party
import nltk
import numpy as np
import pandas as pd
import torch
from geomloss import SamplesLoss
from nltk.util import ngrams
from pydantic import validate_call
from scipy.spatial.distance import jensenshannon
from scipy.special import kl_div
from scipy.stats import chisquare, ks_2samp
from sklearn import metrics
from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import MinMaxScaler

# dga_analysis absolute
import dga_analysis.logger as log
from dga_analysis.datasets.base import BaseDataset
from dga_analysis.utils.serialization import load_from_file, save_to_file

nltk.download("punkt")

DISTANCES = [
    "inv_kl_divergence",
    "ks_test",
    "chi_squared_test",
    # "max_mean_discrepancy",
    "jensenshannon_dist",
    "wasserstein_dist",
    "jaccard_index",
    "cosine_distance",
    "emb_inv_kl_divergence",
    "emb_chi_squared_test",
    "emb_jensenshannon_dist",
    "emb_wasserstein_dist",
    "emb_cosine_distance",
]


def get_summary(arr):
    return {
        "min": float(np.min(arr)),
        "max": float(np.max(arr)),
        "median": float(np.median(arr)),
        "mean": float(np.mean(arr)),
    }


class StatisticalDistances:
    def __init__(self, distances=DISTANCES, only_2ld: bool = True, emb_size: int = 100):
        self.distances = distances
        self._only_2ld = only_2ld
        self._emb_size = emb_size

    def evaluate(self, X_gt: BaseDataset, X_syn: BaseDataset):
        results = {}
        for metric in self.distances:
            if metric == "inv_kl_divergence":
                evaluator = InverseKLDivergence(only_2ld=self._only_2ld)
            elif metric == "ks_test":
                evaluator = KolmogorovSmirnovTest(only_2ld=self._only_2ld)
            elif metric == "chi_squared_test":
                evaluator = ChiSquaredTest(only_2ld=self._only_2ld)
            elif metric == "max_mean_discrepancy":
                evaluator = MaximumMeanDiscrepancy(only_2ld=self._only_2ld)
            elif metric == "jensenshannon_dist":
                evaluator = JensenShannonDistance(only_2ld=self._only_2ld)
            elif metric == "wasserstein_dist":
                evaluator = WassersteinDistance(only_2ld=self._only_2ld)
            elif metric == "jaccard_index":
                evaluator = JaccardIndex(only_2ld=self._only_2ld)
            elif metric == "cosine_distance":
                evaluator = CosineDistance(only_2ld=self._only_2ld)
            elif metric == "emb_inv_kl_divergence":
                evaluator = InverseKLDivergence(
                    only_2ld=self._only_2ld, using_embs=True, emb_size=self._emb_size
                )
            elif metric == "emb_chi_squared_test":
                evaluator = ChiSquaredTest(
                    only_2ld=self._only_2ld, using_embs=True, emb_size=self._emb_size
                )
            elif metric == "emb_jensenshannon_dist":
                evaluator = JensenShannonDistance(
                    only_2ld=self._only_2ld, using_embs=True, emb_size=self._emb_size
                )
            elif metric == "emb_wasserstein_dist":
                evaluator = WassersteinDistance(
                    only_2ld=self._only_2ld, using_embs=True, emb_size=self._emb_size
                )
            elif metric == "emb_cosine_distance":
                evaluator = CosineDistance(
                    only_2ld=self._only_2ld, using_embs=True, emb_size=self._emb_size
                )
            else:
                raise RuntimeError(f"unknown evaluator {metric}")

            results[metric] = evaluator.evaluate(X_gt, X_syn)

        return results


class StatisticalEvaluator:
    def __init__(
        self,
        only_2ld: bool = False,
        using_embs: bool = False,
        emb_size: int = 100,
        **kwargs: Any,
    ) -> None:
        super().__init__(**kwargs)

        self._workspace = Path("workspace")
        self._n_histogram_bins = 10
        self._only_2ld = only_2ld
        self._using_embs = using_embs
        self._emb_size = emb_size

    @staticmethod
    def type() -> str:
        return "stats"

    @abstractmethod
    def _evaluate(self, X_gt: BaseDataset, X_syn: BaseDataset) -> float:
        ...

    @validate_call(config=dict(arbitrary_types_allowed=True))
    def evaluate(self, X_gt: BaseDataset, X_syn: BaseDataset) -> float:
        version = "v2"
        if self._using_embs:
            cache_file = (
                self._workspace
                / f"sc_metric_cache_{self.type()}_emb{self._emb_size}_{self.name()}_{X_gt.hash()}_{X_syn.hash()}.{version}.bkp"
            )

        else:
            cache_file = (
                self._workspace
                / f"sc_metric_cache_{self.type()}_{self.name()}_{X_gt.hash()}_{X_syn.hash()}.{version}.bkp"
            )
        if self.use_cache(cache_file):
            return load_from_file(cache_file)

        results = self._evaluate(X_gt, X_syn)
        print(results)
        save_to_file(cache_file, results)
        return results

    def reduction(self) -> Callable:
        return np.max

    def use_cache(self, path: Path) -> bool:
        return path.exists()

    def get_frequency(
        self, X_gt: pd.DataFrame, X_synth: pd.DataFrame, n_histogram_bins: int = 10
    ) -> dict:
        """Get percentual frequencies for each possible real categorical value.

        Returns:
            The observed and expected frequencies (as a percent).
        """
        res = {}
        for col in X_gt.columns:
            local_bins = min(n_histogram_bins, len(X_gt[col].unique()))

            if len(X_gt[col].unique()) < 5:  # categorical
                gt = (X_gt[col].value_counts() / len(X_gt)).to_dict()
                synth = (X_synth[col].value_counts() / len(X_synth)).to_dict()
            else:
                gt_vals, bins = np.histogram(X_gt[col], bins=local_bins)
                synth_vals, _ = np.histogram(X_synth[col], bins=bins)
                gt = {k: v / (sum(gt_vals) + 1e-8) for k, v in zip(bins, gt_vals)}
                synth = {
                    k: v / (sum(synth_vals) + 1e-8) for k, v in zip(bins, synth_vals)
                }

            for val in gt:
                if val not in synth or synth[val] == 0:
                    synth[val] = 1e-11
            for val in synth:
                if val not in gt or gt[val] == 0:
                    gt[val] = 1e-11

            if gt.keys() != synth.keys():
                raise ValueError(f"Invalid features. {gt.keys()}. syn = {synth.keys()}")
            res[col] = (list(gt.values()), list(synth.values()))

        return res


class InverseKLDivergence(StatisticalEvaluator):
    """
    Returns the average inverse of the Kullbackâ€“Leibler Divergence metric.

    Score:
        0: the datasets are from different distributions.
        1: the datasets are from the same distribution.
    """

    def __init__(self, **kwargs: Any) -> None:
        super().__init__(**kwargs)

    @staticmethod
    def name() -> str:
        return "inv_kl_divergence"

    @staticmethod
    def direction() -> str:
        return "maximize"

    @validate_call(config=dict(arbitrary_types_allowed=True))
    def _evaluate(self, X_gt_raw: BaseDataset, X_syn_raw: BaseDataset) -> float:
        if self._using_embs:
            X_gt, _ = X_gt_raw.as_embeddings(
                only_2ld=self._only_2ld, max_length=self._emb_size
            )
            X_syn, _ = X_syn_raw.as_embeddings(
                only_2ld=self._only_2ld, max_length=self._emb_size
            )
            X_syn = pd.DataFrame(X_syn)
        else:
            X_gt, _ = X_gt_raw.as_statistical(only_2ld=self._only_2ld)
            X_syn, _ = X_syn_raw.as_statistical(only_2ld=self._only_2ld)

        scaler = MinMaxScaler().fit(np.asarray(X_gt))
        X_gt = pd.DataFrame(scaler.transform(np.asarray(X_gt)))
        X_syn = pd.DataFrame(scaler.transform(np.asarray(X_syn)))

        freqs = self.get_frequency(
            X_gt,
            X_syn,
            n_histogram_bins=self._n_histogram_bins,
        )
        res = []
        for col in X_gt.columns:
            gt_freq, synth_freq = freqs[col]
            res.append(1 / (1 + np.sum(kl_div(gt_freq, synth_freq))))

        return get_summary(res)


class KolmogorovSmirnovTest(StatisticalEvaluator):
    """
    Performs the Kolmogorov-Smirnov test for goodness of fit.

    Score:
        0: the distributions are totally different.
        1: the distributions are identical.
    """

    def __init__(self, **kwargs: Any) -> None:
        super().__init__(**kwargs)

    @staticmethod
    def name() -> str:
        return "ks_test"

    @staticmethod
    def direction() -> str:
        return "maximize"

    @validate_call(config=dict(arbitrary_types_allowed=True))
    def _evaluate(self, X_gt: BaseDataset, X_syn: BaseDataset) -> float:
        res = []
        X_gt_stats = X_gt.covariates_statistical(only_2ld=self._only_2ld)
        X_syn_stats = X_syn.covariates_statistical(only_2ld=self._only_2ld)

        scaler = MinMaxScaler().fit(X_gt_stats)
        X_gt_stats = pd.DataFrame(scaler.transform(X_gt_stats))
        X_syn_stats = pd.DataFrame(scaler.transform(X_syn_stats))

        for col in X_syn_stats.columns:
            statistic, _ = ks_2samp(X_gt_stats[col], X_syn_stats[col])
            res.append(1 - statistic)

        return get_summary(res)


class ChiSquaredTest(StatisticalEvaluator):
    """
    Performs the one-way chi-square test.

    Returns:
        The p-value. A small value indicates that we can reject the null hypothesis and that the distributions are different.

    Score:
        0: the distributions are different
        1: the distributions are identical.
    """

    def __init__(self, **kwargs: Any) -> None:
        super().__init__(**kwargs)

    @staticmethod
    def name() -> str:
        return "chi_squared_test"

    @staticmethod
    def direction() -> str:
        return "maximize"

    @validate_call(config=dict(arbitrary_types_allowed=True))
    def _evaluate(self, X_gt: BaseDataset, X_syn: BaseDataset) -> Dict:
        res = []

        if self._using_embs:
            X_gt_stats, _ = X_gt.as_embeddings(
                only_2ld=self._only_2ld, max_length=self._emb_size
            )
            X_syn_stats, _ = X_syn.as_embeddings(
                only_2ld=self._only_2ld, max_length=self._emb_size
            )
        else:
            X_gt_stats = X_gt.covariates_statistical(only_2ld=self._only_2ld)
            X_syn_stats = X_syn.covariates_statistical(only_2ld=self._only_2ld)

        scaler = MinMaxScaler().fit(np.asarray(X_gt_stats))
        X_gt_stats = pd.DataFrame(scaler.transform(np.asarray(X_gt_stats)))
        X_syn_stats = pd.DataFrame(scaler.transform(np.asarray(X_syn_stats)))

        freqs = self.get_frequency(
            X_gt_stats, X_syn_stats, n_histogram_bins=self._n_histogram_bins
        )

        for col in X_gt_stats.columns:
            gt_freq, synth_freq = freqs[col]
            try:
                _, pvalue = chisquare(gt_freq, synth_freq)
                if np.isnan(pvalue):
                    pvalue = 0
            except BaseException:
                log.error("chisquare failed")
                pvalue = 0

            res.append(pvalue)

        return get_summary(res)


class MaximumMeanDiscrepancy(StatisticalEvaluator):
    """
    Empirical maximum mean discrepancy. The lower the result the more evidence that distributions are the same.

    Args:
        kernel: "rbf", "linear" or "polynomial"

    Score:
        0: The distributions are the same.
        1: The distributions are totally different.
    """

    @validate_call(config=dict(arbitrary_types_allowed=True))
    def __init__(self, kernel: str = "rbf", **kwargs: Any) -> None:
        super().__init__(**kwargs)

        self.kernel = kernel

    @staticmethod
    def name() -> str:
        return "max_mean_discrepancy"

    @staticmethod
    def direction() -> str:
        return "minimize"

    @validate_call(config=dict(arbitrary_types_allowed=True))
    def _evaluate(
        self,
        X_gt_raw: BaseDataset,
        X_syn_raw: BaseDataset,
    ) -> Dict:
        if self.kernel == "linear":
            """
            MMD using linear kernel (i.e., k(x,y) = <x,y>)
            """
            delta_df = X_gt_raw.covariates_statistical(only_2ld=self._only_2ld).mean(
                axis=0
            ) - X_syn_raw.covariates_statistical(only_2ld=self._only_2ld).mean(axis=0)
            delta = delta_df.values

            score = delta.dot(delta.T)
        elif self.kernel == "rbf":
            """
            MMD using rbf (gaussian) kernel (i.e., k(x,y) = exp(-gamma * ||x-y||^2 / 2))
            """
            gamma = 1.0
            X_gt = X_gt_raw.covariates_statistical(
                as_numpy=True, only_2ld=self._only_2ld
            )
            X_syn = X_syn_raw.covariates_statistical(
                as_numpy=True, only_2ld=self._only_2ld
            )

            scaler = MinMaxScaler().fit(np.asarray(X_gt))
            X_gt = scaler.transform(np.asarray(X_gt))
            X_syn = scaler.transform(np.asarray(X_syn))

            XX = metrics.pairwise.rbf_kernel(
                X_gt.reshape(len(X_gt), -1),
                X_gt.reshape(len(X_gt), -1),
                gamma,
            )
            YY = metrics.pairwise.rbf_kernel(
                X_syn.reshape(len(X_syn), -1),
                X_syn.reshape(len(X_syn), -1),
                gamma,
            )
            XY = metrics.pairwise.rbf_kernel(
                X_gt.reshape(len(X_gt), -1),
                X_syn.reshape(len(X_syn), -1),
                gamma,
            )
            score = XX.mean() + YY.mean() - 2 * XY.mean()
        elif self.kernel == "polynomial":
            """
            MMD using polynomial kernel (i.e., k(x,y) = (gamma <X, Y> + coef0)^degree)
            """
            degree = 2
            gamma = 1
            coef0 = 0
            X_gt = X_gt_raw.covariates_statistical(
                as_numpy=True, only_2ld=self._only_2ld
            )
            X_syn = X_syn_raw.covariates_statistical(
                as_numpy=True, only_2ld=self._only_2ld
            )

            scaler = MinMaxScaler().fit(np.asarray(X_gt))
            X_gt = scaler.transform(np.asarray(X_gt))
            X_syn = scaler.transform(np.asarray(X_syn))

            XX = metrics.pairwise.polynomial_kernel(
                X_gt.reshape(len(X_gt), -1),
                X_gt.reshape(len(X_gt), -1),
                degree,
                gamma,
                coef0,
            )
            YY = metrics.pairwise.polynomial_kernel(
                X_syn.reshape(len(X_syn), -1),
                X_syn.reshape(len(X_syn), -1),
                degree,
                gamma,
                coef0,
            )
            XY = metrics.pairwise.polynomial_kernel(
                X_gt.reshape(len(X_gt), -1),
                X_syn.reshape(len(X_syn), -1),
                degree,
                gamma,
                coef0,
            )
            score = XX.mean() + YY.mean() - 2 * XY.mean()
        else:
            raise ValueError(f"Unsupported kernel {self.kernel}")

        return {"total": float(score)}


class JensenShannonDistance(StatisticalEvaluator):
    """Evaluate the average Jensen-Shannon distance (metric) between two probability arrays."""

    @validate_call(config=dict(arbitrary_types_allowed=True))
    def __init__(self, normalize: bool = True, **kwargs: Any) -> None:
        super().__init__(**kwargs)

        self.normalize = normalize

    @staticmethod
    def name() -> str:
        return "jensenshannon_dist"

    @staticmethod
    def direction() -> str:
        return "minimize"

    @validate_call(config=dict(arbitrary_types_allowed=True))
    def _evaluate_stats(
        self,
        X_gt_raw: BaseDataset,
        X_syn_raw: BaseDataset,
    ) -> Tuple[Dict, Dict, Dict]:
        stats_gt = {}
        stats_syn = {}
        stats_ = {}

        if self._using_embs:
            X_gt, _ = X_gt_raw.as_embeddings(
                only_2ld=self._only_2ld, max_length=self._emb_size
            )
            X_syn, _ = X_syn_raw.as_embeddings(
                only_2ld=self._only_2ld, max_length=self._emb_size
            )
        else:
            X_gt = X_gt_raw.covariates_statistical(only_2ld=self._only_2ld)
            X_syn = X_syn_raw.covariates_statistical(only_2ld=self._only_2ld)

        scaler = MinMaxScaler().fit(np.asarray(X_gt))
        X_gt = pd.DataFrame(scaler.transform(np.asarray(X_gt)))
        X_syn = pd.DataFrame(scaler.transform(np.asarray(X_syn)))

        for col in X_gt.columns:
            local_bins = min(self._n_histogram_bins, len(X_gt[col].unique()))
            X_gt_bin, gt_bins = pd.cut(X_gt[col], bins=local_bins, retbins=True)
            X_syn_bin = pd.cut(X_syn[col], bins=gt_bins)
            stats_gt[col], stats_syn[col] = X_gt_bin.value_counts(
                dropna=False, normalize=self.normalize
            ).align(
                X_syn_bin.value_counts(dropna=False, normalize=self.normalize),
                join="outer",
                axis=0,
                fill_value=0,
            )
            stats_gt[col] += 1
            stats_syn[col] += 1

            stats_[col] = jensenshannon(stats_gt[col], stats_syn[col])
            if np.isnan(stats_[col]):
                raise RuntimeError("NaNs in prediction")

        return stats_, stats_gt, stats_syn

    @validate_call(config=dict(arbitrary_types_allowed=True))
    def _evaluate(
        self,
        X_gt: BaseDataset,
        X_syn: BaseDataset,
    ) -> Dict:
        stats_, _, _ = self._evaluate_stats(X_gt, X_syn)

        return {"total": sum(stats_.values()) / len(stats_.keys())}


class WassersteinDistance(StatisticalEvaluator):
    """
    Compare Wasserstein distance between original data and synthetic data.

    Args:
        X: original data
        X_syn: synthetically generated data

    Returns:
        WD_value: Wasserstein distance
    """

    def __init__(self, **kwargs: Any) -> None:
        super().__init__(**kwargs)

    @staticmethod
    def name() -> str:
        return "wasserstein_dist"

    @staticmethod
    def direction() -> str:
        return "minimize"

    @validate_call(config=dict(arbitrary_types_allowed=True))
    def _evaluate(
        self,
        X: BaseDataset,
        X_syn: BaseDataset,
    ) -> Dict:
        if self._using_embs:
            X_, _ = X.as_embeddings(only_2ld=self._only_2ld, max_length=self._emb_size)
            X_syn_, _ = X_syn.as_embeddings(
                only_2ld=self._only_2ld, max_length=self._emb_size
            )
        else:
            X_ = X.covariates_statistical(as_numpy=True, only_2ld=self._only_2ld)
            X_syn_ = X_syn.covariates_statistical(
                as_numpy=True, only_2ld=self._only_2ld
            )

        scaler = MinMaxScaler().fit(np.asarray(X_))
        X_ = scaler.transform(np.asarray(X_))
        X_syn_ = scaler.transform(np.asarray(X_syn_))

        X_ = X_.reshape(len(X_), -1)
        X_syn_ = X_syn_.reshape(len(X_syn_), -1)

        if len(X_) > len(X_syn_):
            X_syn_ = np.concatenate(
                [X_syn_, np.zeros((len(X_) - len(X_syn_), X_.shape[1]))]
            )

        X_ten = torch.from_numpy(X_).contiguous()
        Xsyn_ten = torch.from_numpy(X_syn_).contiguous()
        OT_solver = SamplesLoss(loss="sinkhorn")

        return {"total": float(OT_solver(X_ten, Xsyn_ten).cpu().numpy().item())}


class JaccardIndex(StatisticalEvaluator):
    def __init__(self, **kwargs: Any) -> None:
        super().__init__(**kwargs)

    @staticmethod
    def name() -> str:
        return "jaccard_index"

    @staticmethod
    def direction() -> str:
        return "minimize"

    def generalized_jaccard_index(selt, multiset1, multiset2):
        """
        Compute the Generalized Jaccard Index (GJI) for two multisets.

        Parameters:
            multiset1 (list): First multiset.
            multiset2 (list): Second multiset.

        Returns:
            float: Generalized Jaccard Index between the two multisets.
        """
        # Calculate the size of the intersection
        intersection = sum(
            (
                min(multiset1.count(x), multiset2.count(x))
                for x in set(multiset1) & set(multiset2)
            )
        )

        # Calculate the size of the union
        union = sum(
            (
                max(multiset1.count(x), multiset2.count(x))
                for x in set(multiset1) | set(multiset2)
            )
        )

        # Compute the Generalized Jaccard Index
        gji = intersection / union if union != 0 else 0.0

        return gji

    def jaccard_index(self, set1, set2):
        intersection = len(set1.intersection(set2))
        union = len(set1.union(set2))
        return intersection / union if union != 0 else 0

    def _extract_ngrams(self, strings, lens=[2, 3, 4]):
        ngram_list = []
        for string in strings:
            for str_len in lens:
                string_ngrams = list(ngrams(string, str_len))
                ngram_list.extend(string_ngrams)
        return ngram_list

    @validate_call(config=dict(arbitrary_types_allowed=True))
    def _evaluate(
        self,
        X: BaseDataset,
        X_syn: BaseDataset,
    ) -> Dict:
        X_ = X.covariates(only_2ld=self._only_2ld)
        X_syn_ = X_syn.covariates(only_2ld=self._only_2ld)

        testcases = [
            ("2grams", [2]),
            ("23grams", [2, 3]),
            ("234grams", [2, 3, 4]),
        ]
        results = {}
        for testcase, lens in testcases:
            print("eval jaccard index", testcase, flush=True)
            ngrams_X = self._extract_ngrams(X_, lens=lens)
            ngrams_X_syn = self._extract_ngrams(X_syn_, lens=lens)

            results[f"{testcase}_generalized"] = self.generalized_jaccard_index(
                ngrams_X, ngrams_X_syn
            )
            results[f"{testcase}_basic"] = self.jaccard_index(
                set(ngrams_X), set(ngrams_X_syn)
            )

        return results


class CosineDistance(StatisticalEvaluator):
    def __init__(self, **kwargs: Any) -> None:
        super().__init__(**kwargs)

    @staticmethod
    def name() -> str:
        return "cosine_dist"

    @staticmethod
    def direction() -> str:
        return "minimize"

    @validate_call(config=dict(arbitrary_types_allowed=True))
    def _evaluate(
        self,
        X: BaseDataset,
        X_syn: BaseDataset,
    ) -> Dict:
        if self._using_embs:
            X_, _ = X.as_embeddings(only_2ld=self._only_2ld, max_length=self._emb_size)
            X_syn_, _ = X_syn.as_embeddings(
                only_2ld=self._only_2ld, max_length=self._emb_size
            )
        else:
            X_ = X.covariates_statistical(as_numpy=True, only_2ld=self._only_2ld)
            X_syn_ = X_syn.covariates_statistical(
                as_numpy=True, only_2ld=self._only_2ld
            )

        scaler = MinMaxScaler().fit(np.asarray(X_))
        X_ = scaler.transform(np.asarray(X_))
        X_syn_ = scaler.transform(np.asarray(X_syn_))

        model = NearestNeighbors(
            n_neighbors=1, metric="cosine", algorithm="brute", n_jobs=-1
        )
        model.fit(X_)
        distance, _ = model.kneighbors(X_syn_)

        return get_summary(distance)
